{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Showcas/NLP/blob/main/01_2_NLTK_and_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UroXYasai_LV"
      },
      "source": [
        "# Natural Language Processing with Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPuvDbXgi_LY"
      },
      "source": [
        "After understanding the process:\n",
        "\n",
        "**Preprocessing**:\n",
        "\n",
        "- Tokenization\n",
        "- Normalization\n",
        "- Punctuation\n",
        "- ...\n",
        "\n",
        "And after **Feature Extraction**:\n",
        "\n",
        "- Bag-of-Words\n",
        "- ...\n",
        "\n",
        "\n",
        "We will **TRAIN** a classifier with the _features_ **and** the _class_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLZPkCyDi_LZ"
      },
      "source": [
        "# NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfmfLsbVi_La",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "925MewGDi_Lc"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "But first, we need a dataset.\n",
        "\n",
        "We already know `nltk`. There are a few corpora already included, so let's use them for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzDF3nqAi_Lc",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Import the movie_reviews submodule which provides Movie Reviews\n",
        "from nltk.corpus import movie_reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzYdCJE0EtDB"
      },
      "outputs": [],
      "source": [
        "# just like with the stopwords, we need to download this corpus\n",
        "import nltk\n",
        "nltk.download('movie_reviews')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Cyl1ewLi_Ld",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Two possible classes:\n",
        "print(movie_reviews.categories())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5PNEp1HMppD"
      },
      "outputs": [],
      "source": [
        "# getting all fileids for a class\n",
        "movie_reviews.fileids('neg')[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBscDD48M1ch"
      },
      "outputs": [],
      "source": [
        "# getting raw review text\n",
        "movie_reviews.raw('neg/cv000_29416.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "id6nXny_NgRU"
      },
      "outputs": [],
      "source": [
        "# getting review text, already split for us\n",
        "movie_reviews.words('neg/cv000_29416.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qdXILPmi_Ld",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Read all 'neg' reviews\n",
        "neg_files = movie_reviews.fileids(categories=[\"neg\"])\n",
        "\n",
        "neg_reviews = [movie_reviews.raw(fileids=fileid) for fileid in neg_files]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jJHjWeBi_Lf",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Same for 'pos' reviews\n",
        "pos_files = movie_reviews.fileids(categories=[\"pos\"])\n",
        "\n",
        "pos_reviews = [movie_reviews.raw(fileids=fileid) for fileid in pos_files]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQVusp9ji_Lf",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Check sizes:\n",
        "print(f\"#neg: {len(neg_reviews)}\")\n",
        "print(f\"#pos: {len(pos_reviews)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AE8fp7t9i_Lg",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Neg Example:\n",
        "print(f\"Neg:\\n {neg_reviews[42]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYsZx8Cji_Lg",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Pos Example:\n",
        "print(f\"Pos:\\n {pos_reviews[2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9sfZKvQi_Lh"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "We see from the example, that everything is lowercase anyway, so we don't have to deal with this.\n",
        "\n",
        "Also, interpunctuation is divided from its word: No normalization needed.\n",
        "We can see this for example here:\n",
        "\n",
        "> ( it's basically a complete re-shoot of the shop around the corner , only adding a few modern twists ) .\n",
        "\n",
        "There are spaces before and after the brackets, the comma and the period at the end.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "So, we do the following:\n",
        "\n",
        "1. Split on whitespaces\n",
        "2. Remove stopwords and interpunctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSVtvMqFi_Lh",
        "tags": []
      },
      "outputs": [],
      "source": [
        "example = \"This is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit\"\n",
        "\n",
        "# List of all tokens:\n",
        "print(example.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lblt7c2E3bm"
      },
      "outputs": [],
      "source": [
        "# for the tokenization, we will need to download the tokenizer first\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_zz_AeHi_Lh",
        "tags": []
      },
      "outputs": [],
      "source": [
        "example = \"This is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit\"\n",
        "\n",
        "# Now with nltk.word_tokenize:\n",
        "print(nltk.word_tokenize(example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "649asfNHFAeu"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bybNpt_Ki_Lh",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# First, we define the constants\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Use stopwords from NLTK, create a set for faster comparison\n",
        "STOPWORDS = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Add punctuation to stopword set (.union() for UNION)\n",
        "STOPWORDS = STOPWORDS.union(set(punctuation))\n",
        "\n",
        "# Add custom stopwords that we know appear in the text\n",
        "STOPWORDS.add('--')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhFebtK5i_Li",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Define a function for easier access.\n",
        "def tokenization(text):\n",
        "  # Return a list of the \"important\" tokens.\n",
        "  # This is a list comprehension.\n",
        "  #   1. text.split() is called\n",
        "  #   2. Similar to a for loop, the result from 1. is iterated\n",
        "  #   3. The token is added to the final list if its not in the stopword set\n",
        "  #   4. The list is returned\n",
        "  return [token for token in nltk.word_tokenize(text) if token not in STOPWORDS]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnYrw7osh3Jj"
      },
      "outputs": [],
      "source": [
        "# this function is equivalent to the one above\n",
        "# it's easier to read due to not using a list comprehension\n",
        "def tokenization_verbose(text):\n",
        "  tokenized = nltk.word_tokenize(text)\n",
        "  out = []\n",
        "  for token in tokenized:\n",
        "    if token not in STOPWORDS:\n",
        "      out.append(token)\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sN7Y6lWOi_Li",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Example:\n",
        "tokenization(neg_reviews[0])[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hA0Aociiqrz"
      },
      "outputs": [],
      "source": [
        "tokenization(neg_reviews[0]) == tokenization_verbose(neg_reviews[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1XYWcHPi_Li"
      },
      "source": [
        "## Feature Extraction\n",
        "\n",
        "We use a simple Bag-of-Words approach here. So, first we need to create a full vocabulary, so that we can say that each word is a feature. We do that by reading the _full_ corpus (\n",
        "    <strong style='color: #FF6666'>Don't do this later; this is just an example!</strong>\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Laav-E3wi_Li",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "vocabulary = set(\n",
        "    token for text in neg_reviews + pos_reviews for token in tokenization(text)\n",
        ")\n",
        "\n",
        "print(f\"Size of Vocabulary: {len(vocabulary):_}\")\n",
        "\n",
        "# We need the vocabulary as a list\n",
        "vocabulary = sorted(vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_mbcclfi_Lj",
        "tags": []
      },
      "outputs": [],
      "source": [
        "vocabulary[12345]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIyx2aRAi_Lj"
      },
      "source": [
        "Our final list of features **for each** text will be list of 46'289 items: It will be a list of `True` and `False` values, where `True` indicates that the word in the vocabulary list at this position is inside the text.\n",
        "\n",
        "Example: `vocabulary[12345]` is the word `\"donald\"`.\n",
        "If now a text contains this exact word `\"donald\"`, its own feature list will be `True` *at position* `12345`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKvqf2rXi_Lj"
      },
      "source": [
        "```python\n",
        "neg_feature_list = [\n",
        "    [\n",
        "        vocab_item in tokenization(text)\n",
        "        for vocab_item in vocabulary\n",
        "    ]\n",
        "    for text in neg_reviews\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4flXVqLXi_Lk"
      },
      "source": [
        "<strong style='color: #FF6666'>Abort! This takes massively too long and too much storage.</strong>\n",
        "\n",
        "It has to store 50'000 boolean values for each of the 2'000 texts. This is too much.\n",
        "\n",
        "Is there a strategy to reduce the number of features?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUSCLv2Oi_Lk"
      },
      "source": [
        "We could use just 100 words instead of _all of them_. But which ones?\n",
        "\n",
        "- Random ones (does this make sense?)\n",
        "- 100 most common words\n",
        "- ???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGrojS95i_Lk"
      },
      "source": [
        "Let's focus on **2**. But how do we count the number of appearances?\n",
        "\n",
        "We can do this ourselves with a good-oldfashioned dictionary. But there is a module for that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjOOlR6ri_Lk",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Counter will count that what we put in:\n",
        "from collections import Counter\n",
        "\n",
        "print(\"Example:\\n\", Counter([\"a\", \"b\", \"a\", \"c\", \"a\", \"b\", \"d\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJKv6hs2EBhH"
      },
      "source": [
        "A `Counter` is similar to a dictionary: `c['a']` will return the number of appearances of the string `'a'`. Other than a dictionary, it can return 0 if there is no appearance.\n",
        "\n",
        "But wait, there's more:\n",
        "\n",
        "`.most_common(num)`: Returns the top-`num` most occuring tokens as a list of tuples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k2HNjSxEBhH"
      },
      "source": [
        "#### TASK 1.3\n",
        "Count all tokens in all of the texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kn51r6QhEBhI"
      },
      "outputs": [],
      "source": [
        "### IMPLEMENT YOUR SOLUTION HERE ###\n",
        "full_count = Counter(\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rt81RdHHEBhI"
      },
      "outputs": [],
      "source": [
        "# Test the solution and let's see the most common word/token:\n",
        "print(full_count.most_common(10))\n",
        "\n",
        "# The output should be:\n",
        "# [(\"'s\", 18128), ('``', 17625), ('film', 9443), (\"n't\", 6217), ('movie', 5671), ('one', 5582), ('like', 3547), ('even', 2556), ('good', 2316), ('time', 2282)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8im09xvui_Ll",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# It's a tuple! [0] is the token itself, [1] is the number of appearances\n",
        "\n",
        "# Now, we can create the vocabulary out of the 100 most occuring words:\n",
        "\n",
        "vocabulary = sorted(token for token, _ in full_count.most_common(100))\n",
        "\n",
        "print(vocabulary[:50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXBspI9wi_Ll"
      },
      "source": [
        "The **Feature Extraction** is now finished. But the classifier needs a combination of **FEATURES** and the **CLASS**.\n",
        "\n",
        "We call that the _training data_.\n",
        "\n",
        "Usually a list of tuples: (features, class), (features, class), ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2QCch7Ei_Ll"
      },
      "source": [
        "## Classifier\n",
        "\n",
        "Next, we need a classifier that can work with our data.\n",
        "\n",
        "We will see `NLTK`'s version and later the one from `scikit-learn`. Unfortunately, it needs a specific input format.\n",
        "The features per text must be a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FptaF2qi_Lm",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def extract_features(text_tokens):\n",
        "    feature = {}\n",
        "    for word in vocabulary:\n",
        "        feature[f\"contains({word})\"] = word in text_tokens\n",
        "    return feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-ARbD_hi_Lm",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "all_neg_features = [extract_features(tokenization(text)) for text in neg_reviews]\n",
        "\n",
        "all_pos_features = [extract_features(tokenization(text)) for text in pos_reviews]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZR1Tqp-i_Lm",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Example:\n",
        "all_neg_features[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJphu1R6i_Ln",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# To train, we need to attach the LABEL to each feature set:\n",
        "\n",
        "training_data = []  # final list to contain training data tuples\n",
        "\n",
        "# First, we add the features for the 'neg' class:\n",
        "training_data.extend([(feature, \"neg\") for feature in all_neg_features])\n",
        "\n",
        "# Then, we add the features for the 'pos' class:\n",
        "training_data.extend([(feature, \"pos\") for feature in all_pos_features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljWN0C0Ii_Ln",
        "tags": []
      },
      "outputs": [],
      "source": [
        "training_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH7PlErvi_Ln"
      },
      "source": [
        "`NLTK`'s Naive Bayes classifier can now be trained with this information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOpEj38Ei_Lo",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from nltk import NaiveBayesClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rqvgo3ehi_Lo",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# We train by calling the .train() method with the just created training data\n",
        "\n",
        "nb = NaiveBayesClassifier.train(training_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoIrjBYNi_Lp"
      },
      "source": [
        "### Testing\n",
        "\n",
        "How can we test/run the classifier now?\n",
        "\n",
        "For text, we need to do the same things as above:\n",
        "\n",
        "1. Tokenization\n",
        "2. Feature Extraction\n",
        "3. Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOkyIa30i_Lp",
        "tags": []
      },
      "outputs": [],
      "source": [
        "?nb.classify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct5HDmjrEBhP"
      },
      "source": [
        "#### TASK 1.4\n",
        "Write a function to test the classification.\n",
        "1. Tokenize\n",
        "2. Extract features\n",
        "3. Classification with nb.classify()\n",
        "4. Return Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS3KYk3wEBhP"
      },
      "outputs": [],
      "source": [
        "### IMPLEMENT YOUR SOLUTION HERE ###\n",
        "def test_classify(example):\n",
        "\n",
        "    return prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU48tLUli_Lq",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "example = \"I really hate this movie. It is the worst movie that I have ever seen.\"\n",
        "\n",
        "output = test_classify(example)\n",
        "\n",
        "print(f\"The classifier predicts: {output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evWR_Yu7i_Lq",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "example = \"I liked the perfect character performance so much that I watched this great film almost a hundred times back to back and fell fully in love with the well-designed plot.\"\n",
        "\n",
        "output = test_classify(example)\n",
        "\n",
        "print(f\"The classifier predicts: {output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LSobKJui_Lq",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "nb.show_most_informative_features(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9i3xmcZi_Lq"
      },
      "source": [
        "From this view, we can see which *features* contributed to which *class*.\n",
        "\n",
        "Note, this is is purely (!) from the training data, there is nothing of world knowledge or semantics.\n",
        "\n",
        "From the first line, we can see that from the training data, if the text _contains_ the word \"bad\", it is 2 times more likely to be a _negative_ class. The combination of these probabilities lead to the output.\n",
        "\n",
        "The way this works is also by **negative** samples, for example, if the text **DOES NOT** contain the word \"bad\" (`contains(bad) = False`) it is additionally, 1.5 times more probable to belong to the _positive_ class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3RLKUFxi_Lq"
      },
      "source": [
        "## Future Work:\n",
        "\n",
        "Instead of using a simple boolean indicator, we could also use the number of appearances.\n",
        "\n",
        "We could introduce other features, maybe the length of the full text, the average word length, etc. And we could go through the vocabulary and remove more self-defined stopwords (e.g. \"would\" is not on the list). Also, it might make sense to not use the top-100 but the ones in the range between 101-200 or even further down.\n",
        "\n",
        "We'll get to that later."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "HSG_NLP_EDU",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}