{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Showcas/NLP/blob/main/03_2_Galactica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb017879",
      "metadata": {
        "id": "bb017879"
      },
      "source": [
        "# Natural Language Processing with LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "475622f7",
      "metadata": {
        "id": "475622f7"
      },
      "source": [
        "## Introduction to Galactica Models\n",
        "\n",
        "Galactica is a family of large language models optimized for scientific applications. Trained on a specialized corpus, it excels in handling scientific terminology, mathematical expressions, citations, and other usefull things.\n",
        "\n",
        "\n",
        "The Galactica Paper: https://galactica.org/static/paper.pdf\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beed88bc",
      "metadata": {
        "id": "beed88bc"
      },
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "432dafdf",
      "metadata": {
        "id": "432dafdf"
      },
      "source": [
        "To use Galactica, we install the required library and load the model.\n",
        "\n",
        "We recommend using `float16` precision to reduce memory usage when running on Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7e34bbe7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e34bbe7",
        "outputId": "c76725fa-7d47-4ffc-dbe5-7f9bbdcd5b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: galai in /usr/local/lib/python3.11/dist-packages (1.1.6)\n",
            "Requirement already satisfied: torch>=1.12 in /usr/local/lib/python3.11/dist-packages (from galai) (2.5.1+cu124)\n",
            "Requirement already satisfied: transformers==4.25.1 in /usr/local/lib/python3.11/dist-packages (from galai) (4.25.1)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from galai) (0.13.3)\n",
            "Requirement already satisfied: parallelformers==1.2.7 in /usr/local/lib/python3.11/dist-packages (from galai) (1.2.7)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from galai) (1.3.0)\n",
            "Requirement already satisfied: markdown>=3.4 in /usr/local/lib/python3.11/dist-packages (from galai) (3.7)\n",
            "Requirement already satisfied: bleach~=5.0.1 in /usr/local/lib/python3.11/dist-packages (from bleach[css]~=5.0.1->galai) (5.0.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from galai) (5.9.5)\n",
            "Requirement already satisfied: dacite in /usr/local/lib/python3.11/dist-packages (from parallelformers==1.2.7->galai) (1.9.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1->galai) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1->galai) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1->galai) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1->galai) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1->galai) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1->galai) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1->galai) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1->galai) (4.67.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from bleach~=5.0.1->bleach[css]~=5.0.1->galai) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach~=5.0.1->bleach[css]~=5.0.1->galai) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.2,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]~=5.0.1->galai) (1.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12->galai) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.12->galai) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12->galai) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.12->galai) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12->galai) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12->galai) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12->galai) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12->galai) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12->galai) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12->galai) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12->galai) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12->galai) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12->galai) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12->galai) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12->galai) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12->galai) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12->galai) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12->galai) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.12->galai) (1.3.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate->galai) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.12->galai) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.25.1->galai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.25.1->galai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.25.1->galai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.25.1->galai) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install galai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b0446e22",
      "metadata": {
        "id": "b0446e22"
      },
      "outputs": [],
      "source": [
        "import galai as gal\n",
        "from galai.notebook_utils import display_latex\n",
        "from galai.notebook_utils import  display_markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d264c238",
      "metadata": {
        "id": "d264c238"
      },
      "source": [
        "After installation, you can obtain an overview of the available model sizes with the following."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9ebb04aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "9ebb04aa",
        "outputId": "f124333e-820e-40cc-996c-55ee1658f9a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ModelInfo(name='mini', num_layers=12, num_heads=12, head_size=64, vocab_size=50000, max_positions=2048),\n",
              " ModelInfo(name='base', num_layers=24, num_heads=32, head_size=64, vocab_size=50000, max_positions=2048),\n",
              " ModelInfo(name='standard', num_layers=32, num_heads=32, head_size=128, vocab_size=50000, max_positions=2048),\n",
              " ModelInfo(name='large', num_layers=48, num_heads=56, head_size=128, vocab_size=50000, max_positions=2048),\n",
              " ModelInfo(name='huge', num_layers=96, num_heads=80, head_size=128, vocab_size=50000, max_positions=2048)]"
            ],
            "text/html": [
              "<table><thead><tr><th>Name</th><th>Parameters</th><th>Layers</th><th>Heads</th><th>Head Size</th><th>Vocabulary Size</th><th>Context Size</th></tr></thead><tbody><tr><td><strong>mini</strong></td><td>125.0 M</td><td>12</td><td>12</td><td>64</td><td>50000</td><td>2048</td></tr><tr><td><strong>base</strong></td><td>1.3 B</td><td>24</td><td>32</td><td>64</td><td>50000</td><td>2048</td></tr><tr><td><strong>standard</strong></td><td>6.7 B</td><td>32</td><td>32</td><td>128</td><td>50000</td><td>2048</td></tr><tr><td><strong>large</strong></td><td>30.0 B</td><td>48</td><td>56</td><td>128</td><td>50000</td><td>2048</td></tr><tr><td><strong>huge</strong></td><td>121.3 B</td><td>96</td><td>80</td><td>128</td><td>50000</td><td>2048</td></tr></tbody></table>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from galai.utils import ModelInfo\n",
        "ModelInfo.all()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fc4e87e",
      "metadata": {
        "id": "2fc4e87e"
      },
      "source": [
        "With 16-bit floating-point precission (FP16) we can estimate the model footprint of the different model sizes:\n",
        "\n",
        "\n",
        "| Model     | Parameters  | Size (FP16) |\n",
        "|-----------|------------|------------|\n",
        "| **Mini**     | 125M  | **0.25 GB** |\n",
        "| **Base**     | 1.3B  | **2.6 GB**  |\n",
        "| **Standard** | 6.7B  | **13.4 GB** |\n",
        "| **Large**    | 30B   | **60 GB**   |\n",
        "| **Huge**     | 121.3B | **242.6 GB** |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47261725",
      "metadata": {
        "id": "47261725"
      },
      "source": [
        "Depending on how much computing capacity your system has available, you can choose the model size accordingly. The `standard` model is selected by default. If this consumes too many resources, switch to the `base` or `mini` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "162dccfe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382,
          "referenced_widgets": [
            "f4dbc88916924c619d346ee2470ad469",
            "e22258979a774e2f9a99725b485b6696",
            "93ea3f93c3c84a1b89750032d17213d5",
            "e87b0e1818bc44c4bbaee9b53aee1f12",
            "ece194e90bb241df8a8d2eb4f3117e65",
            "e6acf234eb484deebca421cdac357618",
            "3db0b4153d144e13a7c1ff54d3c03573",
            "85fff5bc87554bfc991bf949fd34404d",
            "906a26a77fbc4128a84f2482822b65fa",
            "2005d1f59dde4cdabae162fed2f11035",
            "6b1a321f482d4b2984a1bcbc8ce4fe4d",
            "63a63be020bd44d781088e40150b3547",
            "19c680ba057744dbb8e9eac5b6e833df",
            "cc3ac398e9fb4068a5ffb0b2679c3c75",
            "ef52c5e249cd41698dd8c6ca45958bc8",
            "a723c87ac48746bb8b65e51ed9046e06",
            "3683c2af1fff4db6bcab3fb37be5842f",
            "54f1b44f47754f97b1accacd287051e1",
            "642b00940706498fbcfe236045e9e453",
            "c957c5077e0d4b8988b052467736ea70",
            "8ff2a567701a44749249a2ab7e2c015e",
            "495b90022517498e8c8b859d903b68b3",
            "c1b135c9bd834ae2b26746a447a36d9f",
            "e8c395a26ea64f468108d86e8ac2539f",
            "316a4182f0244d499606c59c468109c2",
            "75e4f2c1380a40ffac162d92b60d605b",
            "784d251db7db42f28f191a4d244b484c",
            "43f9bfcecd35434fb66384841d00ca53",
            "42fadf6cb06245be81135ba22a6b5731",
            "0a7abe5f063d47feaf388252e2d5d7b7",
            "04933256654b4983a62e8eee9d3bdbf8",
            "a25a338a6e48480bb0b62c90d0e3b487",
            "4eb073de9e5e4b95a94b436853295834",
            "054187455bb7443dbd6706318a42191e",
            "07c81e3bc2264037a78e6986a1daa55b",
            "4f8271f93ff64572a20b8438dfc42582",
            "abe386171f024ec2b129e55d57a48eef",
            "3aee636048ca41dda07c9a55f39d0184",
            "5677475bde1e43a1936cfc18ce80e368",
            "673d30aec9bb49918f2815d2fe37d1e2",
            "208d3bc2438345f8a2cc1f7acaa760b7",
            "75f75acaddf54d298bb1f5b14ff027a8",
            "e3a18dfd271442469acd256b59dc94cc",
            "27ef536f5fb54899ab0a5cbd427e308a",
            "baf7d925e7404aafaca7136b7be20711",
            "b2ca0b2971ec44e28ccb70d51833546b",
            "6e0419b7d2fc46298fc53753560d23bd",
            "4178e5ff6e8c4339b7f58c5d32b9a141",
            "634dc4c474404aeb83ef9a9d97be43dd",
            "23e960d31dce42a88217615dce93b241",
            "ab57986e44eb4e2a883771bad50b703b",
            "31e4578b04bf400eadeffccaf70cc14d",
            "52631027568c4daeb1345c58860cf9ef",
            "31cec14f0ee24582a97c88f189afd2a1",
            "f7e5bff20b664603b5f4cb0100e38bce"
          ]
        },
        "id": "162dccfe",
        "outputId": "1b463f94-cdb0-4e08-c42b-e76850a54c8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/166 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4dbc88916924c619d346ee2470ad469"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.14M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63a63be020bd44d781088e40150b3547"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/3.00 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1b135c9bd834ae2b26746a447a36d9f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/789 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "054187455bb7443dbd6706318a42191e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "baf7d925e7404aafaca7136b7be20711"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
          ]
        }
      ],
      "source": [
        "# Load a smaller model for Colab compatibility\n",
        "model = gal.load_model(\n",
        "    name=\"base\",  # change this for other model sizes (e.g. \"base\")\n",
        "    dtype=\"float16\",\n",
        "    # num_gpus=None,    # If you have multiple GPUs, you can specify the number of GPUs to use\n",
        "    parallelize=None, # This will parallelize the model across multiple GPUs\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f576b5a6",
      "metadata": {
        "id": "f576b5a6"
      },
      "source": [
        "**Remark:** Using a smaller modelsize than standard can yield to bad examples in the following notebook. It is highly recommended to use the biggest model you can and feel free to experiment with different model sizes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cbb949f",
      "metadata": {
        "id": "6cbb949f"
      },
      "source": [
        "## 2. Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb9804ae",
      "metadata": {
        "id": "eb9804ae"
      },
      "source": [
        "Galactica can generate high-quality scientific text based on prompts. This is useful for research papers, blog posts, or academic discussions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f5da0949",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5da0949",
        "outputId": "1de60191-e761-49d8-cef4-141d6627f911"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is Natural Language Processing?, Manning[END_REF]).\n",
            "\n",
            "# 2.2.2.2.2.3.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2\n"
          ]
        }
      ],
      "source": [
        "prompt = \"What is Natural Language Processing?\"\n",
        "output = model.generate(prompt, max_new_tokens=100)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0feff2cb",
      "metadata": {
        "id": "0feff2cb"
      },
      "source": [
        "## 3. Generating Citations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ec8a00e",
      "metadata": {
        "id": "6ec8a00e"
      },
      "source": [
        "Galactica has been trained to recognize and suggest citations using `[START_REF]` tokens.\n",
        "\n",
        "This feature can help researchers find references without manual searches.\n",
        "\n",
        "Keep in Mind, Galactica was trained in 2022 and therefore also has a knowledge cut of this date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cb2ea5ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "cb2ea5ea",
        "outputId": "668aacbb-7594-43cc-e386-7a8ece4f26db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Transformer architecture [START_REF] Attention is All you Need, Vaswani[END_REF] is a popular choice for sequence-to-sequence models. It consists of a stack of encoder and decoder layers, each of which is composed of a multi-head self-attention mechanism and a feed-forward network. The encoder is used to encode the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "model.generate(\"The Transformer architecture [START_REF]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82fb30dc",
      "metadata": {
        "id": "82fb30dc"
      },
      "source": [
        "If we ask the model for more recent papers, we get, as with many LLMs, hallucinated results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "da791504",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "da791504",
        "outputId": "8e21dd2c-363e-4dbc-ed4f-ee857b350147"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The new LLM DeepSeek [START_REF] DeepSeek: A New Multi-Scale Deep Convolutional Neural Network for Fast Image Retrieval, Liu[END_REF] is a multi-scale CNN that uses a multi-scale feature pyramid to extract features from the input image. The features are then fed into a fully connected layer to generate a feature vector. The feature'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "model.generate(\"The new LLM DeepSeek [START_REF]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "698ef7dc",
      "metadata": {
        "id": "698ef7dc"
      },
      "source": [
        "## 4. Handling Mathematical Expressions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4332f088",
      "metadata": {
        "id": "4332f088"
      },
      "source": [
        "Galactica was trained also on math formulars and can generate and understand LaTeX equations, which is beneficial for scientific writing. The framework even has its own LaTeX and markdown display functions that make it possible to have appropriate formatting in notebooks. (Due to different LaTeX standards, the output of math formulas in local jupyter notebooks does not work sometimes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ed27b0e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ed27b0e3",
        "outputId": "607c02c5-c28c-4c5e-bbd5-39f536eca4f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The Riemann zeta function is given by: $$\\zeta(s)=\\sum_{n=1}^{\\infty}\\frac{1}{n^{s}}$$.\n",
              "\n",
              "The Riemann hypothesis states that the zeros of the zeta function are on the critical line $\\Re(s)=\\frac{1}{2}$.\n",
              "\n",
              "The Riemann hypothesis is a major open problem in mathematics.\n",
              "\n",
              "## See also\n",
              "\n",
              "* Dirichlet series\n",
              "* Dirichlet's"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from galai.notebook_utils import display_latex\n",
        "\n",
        "math_prompt = \"The Riemann zeta function is given by: \\\\[\"\n",
        "math_output = model.generate(math_prompt, max_new_tokens=100)\n",
        "display_latex(math_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "500e1737",
      "metadata": {
        "id": "500e1737"
      },
      "source": [
        "#### new_doc = True:\n",
        "Galactica was trained on scientific documents that were separated using `</s>`, which marks the end of one document and the beginning of another. By adding `</s>` before your prompt, it signals to the model that this is the start of a new document rather than a continuation.\n",
        "\n",
        "Without the parameter, the model treats the prompt as part of an ongoing text, possibly completing an unfinished sentence. To prevent this, we set `new_doc = True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7145d7d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "7145d7d6",
        "outputId": "71cf4046-b660-4fe4-daec-d756272c3fc6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h1>The Hitchhiker's Guide to the Galaxy (novel) Wikipedia Entry</h1>\n",
              "<p>The Hitchhiker's Guide to the Galaxy is a 1998 science fiction novel by American writer Robert A. Heinlein. It is the third book in the Hitchhiker's Guide to the Galaxy series, and the second novel in the series to be published by Tor Books.</p>\n",
              "<p>The Hitchhiker's Guide to the Galaxy </p>\n",
              "<h2>Plot summary</h2>\n",
              "<p>The novel opens with the arrival of the Hitchhiker's Guide to the Galaxy, a group of scientists and engineers who have been sent to the galaxy to investigate the mysterious \"Great Darkness\". The guide is led by Dr. John \"J.J.\" Jones, a scientist who has been sent to investigate the mysterious \"Great Darkness\". The guide is accompanied by Dr. John \"J.J.\" Jones, a scientist who has been sent to investigate the mysterious \"Great Darkness\". The guide is accompanied by Dr</p>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from galai.notebook_utils import  display_markdown\n",
        "display_markdown(model.generate(\"# The Hitchhiker's Guide to the Galaxy (novel) Wikipedia Entry\\n\\n\", new_doc=True,  max_new_tokens=200))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e449279c",
      "metadata": {
        "id": "e449279c"
      },
      "source": [
        "## 5. Step-by-Step Reasoning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31c32e71",
      "metadata": {
        "id": "31c32e71"
      },
      "source": [
        "The `<work>` token guides Galactica through logical step-by-step reasoning. This is useful for breaking down complex problems into smaller logical steps.\n",
        "\n",
        "Note, this was befor reasoning models and was all achieved through clever training and special tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d578ee86",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d578ee86",
        "outputId": "03cc0ff4-51d8-44ba-c340-5d340b251b53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost?\n",
            "\n",
            "<work>\n",
            "\n",
            "The bat costs $1.00 more than the ball.\n",
            "\n",
            "calc_1.py\n",
            "```\n",
            "result = 1.00+1.10\n",
            "\n",
            "with open(\"output.txt\", \"w\") as file:\n",
            "    file.write(str(round(result)))\n",
            "```\n",
            "\n",
            "<<run: \"calc_1.py\">>\n",
            "\n",
            "<<read: \"output.txt\">>\n"
          ]
        }
      ],
      "source": [
        "reasoning_prompt = \"A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost?\\n\\n<work>\"\n",
        "reasoning_output = model.generate(reasoning_prompt, max_new_tokens=100)\n",
        "print(reasoning_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd82ea9a",
      "metadata": {
        "id": "dd82ea9a"
      },
      "source": [
        "**Remark**: The Huge model solves the this Task correctly to **0.05** when using the step-by-step reasoning. The model size plays a decisive role in resoning in particular."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9ea6a8fd",
      "metadata": {
        "id": "9ea6a8fd"
      },
      "outputs": [],
      "source": [
        "output = model.generate(\n",
        "        \"What is the $7$-th harmonic number of the second order? Answer with Python source code.\\n\\n<work>\",\n",
        "        max_new_tokens=700,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "792e0eba",
      "metadata": {
        "id": "792e0eba"
      },
      "source": [
        "This should have a reasoning format as output where a python code for the calculation of the problem is generated.\n",
        "\n",
        "for example like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f1256d04",
      "metadata": {
        "id": "f1256d04"
      },
      "outputs": [],
      "source": [
        "# A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost?\n",
        "\n",
        "# <work>\n",
        "\n",
        "# The bat costs $1.00 more than the ball, so the ball costs $1.00 + $1.10\n",
        "\n",
        "# calc_1.py\n",
        "# ```\n",
        "# result = 1.00+1.10\n",
        "\n",
        "# with open(\"output.txt\", \"w\") as file:\n",
        "#     file.write(str(round(result)))\n",
        "# ```\n",
        "\n",
        "# <<run: \"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09c96118",
      "metadata": {
        "id": "09c96118"
      },
      "source": [
        "#### TASK 3.3\n",
        "\n",
        "Implement a function that extracts the code block from the Glalactica reasoning output and executes it. Note that not every reasoning output from galactica also contains a codeblock."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "69cbf110",
      "metadata": {
        "id": "69cbf110"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def calculate_reasoning(model_output):\n",
        "    ### IMPLEMENT YOUR SOLUTION HERE ###\n",
        "\n",
        "    match = re.search(r'```(.*?)```', model_output, re.DOTALL)\n",
        "\n",
        "    if match:\n",
        "        code_block = match.group(1).strip()\n",
        "\n",
        "        try:\n",
        "            local_variables = {}\n",
        "            exec(code_block, {}, local_variables)\n",
        "\n",
        "            return local_variables\n",
        "        except Exception as e:\n",
        "            return f\"Error in code: {str(e)}\"\n",
        "    else:\n",
        "        return \"No code block found.\"\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "5a50411a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a50411a",
        "outputId": "eb7095db-c1e0-4cfc-c85f-f9b57e53389f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'result': 2.1}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "test_output = 'A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost?\\n\\n<work>\\n\\nThe bat costs $1.00 more than the ball, so the ball costs $1.00 + $1.10 \\n\\ncalc_1.py\\n```\\nresult = 1.00+1.10\\n\\nprint(result)\\n```\\n\\n<<run: \"'\n",
        "calculate_reasoning(test_output)\n",
        "\n",
        "## this should print 2.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac4b317d",
      "metadata": {
        "id": "ac4b317d"
      },
      "source": [
        "Lets try a harder problem where more code is required. This requires a bit more computing power since we need more tokens to generate the complete calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2adc774e",
      "metadata": {
        "id": "2adc774e"
      },
      "outputs": [],
      "source": [
        "output = model.generate(\n",
        "        \"What is the $7$-th harmonic number of the second order? Answer with Python source code.\\n\\n<work>\",\n",
        "        max_new_tokens=500,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "77c2b751",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77c2b751",
        "outputId": "ad2f39c7-59b4-420c-9e8b-9f7962826d9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the $7$-th harmonic number of the second order? Answer with Python source code.\n",
            "\n",
            "<work>\n",
            "\n",
            "The $7$-th harmonic number of the second order is $\\dfrac{1}{2}\\left(\\dfrac{1}{2}+\\dfrac{1}{4}+\\dfrac{1}{8}+\\dfrac{1}{16}+\\dfrac{1}{32}+\\dfrac{1}{64}\\right)$.\n",
            "\n",
            "Let's find the $7$-th harmonic number of the first order.\n",
            "\n",
            "$\\begin{aligned} \\dfrac{1}{2}\\left(\\dfrac{1}{2}+\\dfrac{1}{4}+\\dfrac{1}{8}+\\dfrac{1}{16}+\\dfrac{1}{32}+\\dfrac{1}{64}\\right) &= \\dfrac{1}{2}\\left(\\dfrac{1}{2}+\\dfrac{1}{4}+\\dfrac{1}{8}+\\dfrac{1}{16}+\\dfrac{1}{32}+\\dfrac{1}{64}\\right)+\\dfrac{1}{2}\\left(\\dfrac{1}{2}+\\dfrac{1}{4}+\\dfrac{1}{8}+\\dfrac{1}{16}+\\dfrac{1}{32}+\\dfrac{1}{64}\\right) \\\\\\\\ &= \\dfrac{1}{2}\\left(\\dfrac{1}{2}+\\dfrac{1}{4}+\\dfrac{1}{8}+\\dfrac{1}{16}+\\dfrac{1}{32}+\\dfrac{1}{64}\\right)+\\dfrac{1}{2}\\left(\\dfrac{1}{2}+\\dfrac{1}{4}+\\dfrac{1}{8}+\\dfrac{1}{16}+\\dfrac{1}{32}+\\dfrac{1}{64}\\right) \\\\\\\\ &= \\dfrac{1}{2}\\left(\n"
          ]
        }
      ],
      "source": [
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "7f9a56c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7f9a56c1",
        "outputId": "1408fcba-6f84-4074-c1f3-5970eb6d828a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'No code block found.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "calculate_reasoning(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e795a91",
      "metadata": {
        "id": "2e795a91"
      },
      "source": [
        "## 6. Text Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9338b607",
      "metadata": {
        "id": "9338b607"
      },
      "source": [
        "Galactica can generate concise summaries of longer texts, making it useful for academic papers and research abstracts. For summarization we use the `\\n\\nTLDR:` in our prompt to generate the desired summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "5be26f4a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5be26f4a",
        "outputId": "7638f974-b99c-41e0-d378-43d093eec4a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.\n",
            "\n",
            "TLDR: We introduce Galactica, a large language model that can store, combine and reason about scientific knowledge. We outperform existing models on a range of scientific tasks.\n",
            "\n",
            "# 1 Introduction\n",
            "\n",
            "The explosion of scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason\n"
          ]
        }
      ],
      "source": [
        "TEXT = \"\"\"Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.\"\"\"\n",
        "summary = model.generate(TEXT + \"\\n\\nTLDR:\", max_new_tokens=100)\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3437f2a",
      "metadata": {
        "id": "f3437f2a"
      },
      "source": [
        "## 7. Sentiment Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30212772",
      "metadata": {
        "id": "30212772"
      },
      "source": [
        "With some guidance, the model can also determine the sentiment by logical combination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "0b49542a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "0b49542a",
        "outputId": "79b971e2-bbc5-4959-e0f5-87b29ae5e74c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p>Post: \"I hate it when my phone battery dies.\"\n",
              "Sentiment: Negative</p>\n",
              "<h3></h3>\n",
              "<p>Post: \"My day has been 👍\"\n",
              "Sentiment: Positive</p>\n",
              "<h3></h3>\n",
              "<p>Post: \"This is the link to the article\"\n",
              "Sentiment: Neutral</p>\n",
              "<h3></h3>\n",
              "<p>Post: \"This new music video was incredibile\"\n",
              "Sentiment: Positive</p>\n",
              "<h3></h3>\n",
              "<p>Post: </p>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "prompt = \"\"\"Post: \"I hate it when my phone battery dies.\"\n",
        "Sentiment: Negative\n",
        "###\n",
        "Post: \"My day has been 👍\"\n",
        "Sentiment: Positive\n",
        "###\n",
        "Post: \"This is the link to the article\"\n",
        "Sentiment: Neutral\n",
        "###\n",
        "Post: \"This new music video was incredibile\"\n",
        "Sentiment:\"\"\"\n",
        "output=model.generate(prompt, max_length=90)\n",
        "display_markdown(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "1f98e50d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "1f98e50d",
        "outputId": "69f56fec-c1aa-42e6-a174-395012fb27c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p>Movie Review: \"The movie was great, but the ending was disappointing.\"\n",
              "Sentiment: Negative</p>\n",
              "<p>Movie Review: \"The movie was a bit slow, but the acting was good.\"\n",
              "Sentiment: Neutral</p>\n",
              "<p>Movie Review: \"The movie was amazing, I loved it!\"\n",
              "Sentiment: Positive</p>\n",
              "<p>Movie Review: \"The movie was terrible, I hated it.\"\n",
              "Sentiment: Negative</p>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "prompt = \"\"\"Movie Review: \"The movie was great, but the ending was disappointing.\"\n",
        "Sentiment: Negative\n",
        "\\n\n",
        "Movie Review: \"The movie was a bit slow, but the acting was good.\"\n",
        "Sentiment: Neutral\n",
        "\\n\n",
        "Movie Review: \"The movie was amazing, I loved it!\"\n",
        "Sentiment: Positive\n",
        "\\n\n",
        "Movie Review: \"The movie was terrible, I hated it.\"\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "output=model.generate(prompt, max_new_tokens=3)\n",
        "display_markdown(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55653b76",
      "metadata": {
        "id": "55653b76"
      },
      "source": [
        "## 8. Question-Answering Knowledge"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cf866a0",
      "metadata": {
        "id": "5cf866a0"
      },
      "source": [
        "Galactica can answer technical and scientific questions by retrieving stored knowledge. It also tries to combine knowledge with references marked with the tokens for `[START_REF]` and `[END_REF]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "bcfb3920",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcfb3920",
        "outputId": "6cd2eb96-8a62-4aec-e1a9-34e9ef444223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is a Transformer model?\n",
            "\n",
            "Answer: A Transformer is a type of neural network that is used to model sequential data. It is a type of recurrent neural network (RNN).\n",
            "\n",
            "A Transformer is a type of neural network that is used to model sequential data. It is a type of recurrent neural network (RNN).\n",
            "\n",
            "A Transformer is a type of neural network that is used to model sequential data. It is a type of recurrent neural network (RNN).\n",
            "\n",
            "A Transformer is a type of neural\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Question: What is a Transformer model?\\n\\nAnswer:\"\n",
        "output = model.generate(prompt, max_new_tokens=100)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d18b866",
      "metadata": {
        "id": "9d18b866"
      },
      "source": [
        "The multiple modalities that Galactica is able to work with allows us to query for papers using math, source code, etc.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "19e48d37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "19e48d37",
        "outputId": "523dce5f-cdab-4464-c759-ecca10970d47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p><strong>Prompt</strong>: The paper that presented a novel computing block given by the formula:\n",
              "$$\n",
              "f(Q, K, V) = \\textrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
              "$$</p>\n",
              "<p><strong>Reference</strong>: Attention is All you Need, Vaswani</p>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "prompt = \"\"\"The paper that presented a novel computing block given by the formula:\n",
        "\\\\[\n",
        "f(Q, K, V) = \\\\textrm{softmax}\\\\left(\\\\frac{QK^T}{\\\\sqrt{d_k}}\\\\right)V\n",
        "\\\\]\n",
        "\n",
        "\"\"\"\n",
        "reference = model.generate_reference(prompt)\n",
        "display_markdown(f\"**Prompt**: {prompt}\\n\\n**Reference**: {reference}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "a3fb24a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "a3fb24a6",
        "outputId": "ff70ce04-1482-48ab-b3f0-f59acb68cc8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p><strong>Prompt</strong>:</p>\n",
              "<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">while</span> <span class=\"n\">k</span> <span class=\"o\">&gt;</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n",
              "    <span class=\"k\">if</span> <span class=\"n\">k</span> <span class=\"o\">%</span> <span class=\"mi\">2</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n",
              "        <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">k</span> <span class=\"o\">//</span> <span class=\"mi\">2</span>\n",
              "    <span class=\"k\">else</span><span class=\"p\">:</span>\n",
              "        <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"mi\">3</span> <span class=\"o\">*</span> <span class=\"n\">k</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n",
              "</code></pre></div>\n",
              "\n",
              "<p>A paper studying if the loop above terminates for all positive integers </p>\n",
              "<p><strong>Reference</strong>: On the number of iterations of the loop for computing the nth prime, Kourbatov</p>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "prompt = \"\"\"```python\n",
        "while k > 1:\n",
        "    if k % 2 == 0:\n",
        "        k = k // 2\n",
        "    else:\n",
        "        k = 3 * k + 1\n",
        "```\n",
        "\n",
        "A paper studying if the loop above terminates for all positive integers \"\"\"\n",
        "reference = model.generate_reference(prompt)\n",
        "display_markdown(f\"**Prompt**:\\n{prompt}\\n\\n**Reference**: {reference}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64be454f",
      "metadata": {
        "id": "64be454f"
      },
      "source": [
        "You can get multiple suggestions of reference for a given prompt by setting `suggestions` parameter. With `suggestions > 1` a beam search decoding is used to try to generate more suggestions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "40f8796a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40f8796a",
        "outputId": "b441073e-72a4-4d78-de39-6fcb9311fb9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural Language Processing (Almost) from Scratch, Collobert\n",
            "A survey of deep neural network architectures and their applications, Liu\n",
            "A survey of deep neural network architectures and their applications, Liu\n",
            "A survey of deep neural network architectures and their applications, Liu\n",
            "A survey of deep neural network architectures and their applications, Liu\n"
          ]
        }
      ],
      "source": [
        "for reference in model.generate_reference(\"A survey paper on nlp models\",suggestions=5):\n",
        "    print(reference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "633e3ed8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633e3ed8",
        "outputId": "8b578a33-4f42-4c72-b083-0b76c3020c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/beam_search.py:198: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect. `max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`, or `group_beam_search(...)`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural Language Processing (Almost) from Scratch, Collobert\n",
            "Neural Network Models for Natural Language Processing, Goldberg\n",
            "A Survey of Neural Network Models for Natural Language Processing, Goldberg\n",
            "Survey on Natural Language Processing, Joshi\n",
            "A survey on natural language processing models, Soni\n"
          ]
        }
      ],
      "source": [
        "for reference in model.generate_reference(\n",
        "    \"A survey paper on nlp models\",\n",
        "    suggestions=5, diversity_penalty=0.9\n",
        "):\n",
        "    print(reference)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8fd8e70",
      "metadata": {
        "id": "c8fd8e70"
      },
      "source": [
        "This example shows how important the choice of tokens is during generation in addition to model training and prompting. The geedy method is not always the best way to select the next token. More complex search algorithms may take a little longer, but they produce significantly better results.\n",
        "\n",
        "It is comparable to a game of chess. The player who only ever thinks one move ahead will not play as well as a player who takes the next 5 moves or more into account.\n",
        "\n",
        "We will learn more about that in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0a2377c",
      "metadata": {
        "id": "d0a2377c"
      },
      "source": [
        "## 9. Text Generation Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74b51f1b",
      "metadata": {
        "id": "74b51f1b"
      },
      "source": [
        "Galactica uses the Huggingface library and with that we can compare different sampling methods for the text generation. We compare them with the following prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "d9e21cd6",
      "metadata": {
        "id": "d9e21cd6"
      },
      "outputs": [],
      "source": [
        "prompt = \"Title: A Literature Review on Sentiment Analysis\\n\\n# Abstract\\n\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2c8f7e7",
      "metadata": {
        "id": "b2c8f7e7"
      },
      "source": [
        "### Greedy Decoding\n",
        "\n",
        "This is the standard algorithm used by `Model.generate`. Using the prompt and already generated tokens, the model computes a probability distribution of the next token over all tokens. The token with the highest score is appended to the generated text and the process is repeated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "2684cce9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "2684cce9",
        "outputId": "dc5ec17c-7668-46ee-fed9-bd33b9563d7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p>Title: A Literature Review on Sentiment Analysis</p>\n",
              "<h1>Abstract</h1>\n",
              "<p>Sentiment analysis is a field of research that deals with the analysis of the opinions, emotions, and sentiments of people. It is a very important field of research in the field of natural language processing. It is a very useful tool for analyzing the opinions of people and their sentiments. It is used in many fields such as marketing, politics, and social media. The main objective of this paper is to review the various techniques used for sentiment analysis. The paper also discusses the various datasets used for sentiment analysis. The paper also discusses the various techniques used for sentiment analysis. The paper also discusses the various datasets used for sentiment analysis. The paper also discusses the various techniques used for sentiment analysis. The paper also discusses the various datasets used</p>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "output = model.generate(prompt, max_new_tokens=150)\n",
        "display_markdown(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cced0f9a",
      "metadata": {
        "id": "cced0f9a"
      },
      "source": [
        "### Beam Search\n",
        "\n",
        "In Beam Search, the model computes a probability distribution of the next token over all tokens for each of the `num_beams` generated sequences. The `num_beams` sequences with the highest probability are kept and the process is repeated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "fe10ab74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "fe10ab74",
        "outputId": "fdeef89c-d8a6-4b8a-8983-6644c2b7da6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p>Title: A Literature Review on Sentiment Analysis</p>\n",
              "<h1>Abstract</h1>\n",
              "<p>Sentiment analysis (SA) is one of the most important research areas in the field of natural language processing (NLP). It deals with the classification of text into positive, negative, and neutral classes based on the sentiment expressed in the text. In this paper, we present a literature review on the state-of-the-art techniques used for sentiment analysis. The paper is organized as follows. In Section 2, we present a brief introduction to sentiment analysis. In Section 3, we present a literature review on the state-of-the-art techniques used for sentiment analysis. Finally, we conclude the paper in Section 4.</p>\n",
              "<h1>1. Introduction</h1>\n",
              "<p>Sentiment analysis (SA) is</p>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "output = model.generate(prompt, num_beams=5, max_new_tokens=150)\n",
        "display_markdown(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3bf27e6",
      "metadata": {
        "id": "e3bf27e6"
      },
      "source": [
        "You can return up to `num_beams` sequences by specifying `num_return_sequences`.\n",
        "\n",
        "Beam search is slower and requires more memory compared to the Greedy Decoding. The increase in memory consumption is proportional to the number of beams used."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "423fa13d",
      "metadata": {
        "id": "423fa13d"
      },
      "source": [
        "### Contrastive Search\n",
        "\n",
        "The contrastive search ([Su et al.](https://arxiv.org/abs/2202.06417), [Su et al.](https://arxiv.org/abs/2210.14140)) algorithm is a novel generation method that aims to produce more natural texts by penalizing repetitions. We can use `transformers` implementation (see more at https://huggingface.co/blog/introducing-csearch) by specifying `penalty_alpha` and `top_k`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "0b036c37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "0b036c37",
        "outputId": "be0fecf8-b1b5-4a51-ac4a-38fb1ef9522d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p>Title: A Literature Review on Sentiment Analysis</p>\n",
              "<h1>Abstract</h1>\n",
              "<p>Sentiment analysis is a process of extracting information about the attitude of the user towards an object or event from the text. Sentiment analysis is used for many applications such as marketing, product reviews, social media, and customer service. The main aim of this study is to review the existing literature in sentiment analysis. It also provides an overview of the existing methods and techniques for performing sentiment analysis. The study is conducted using the keywords ‘sentiment analysis’, ‘text analysis’, ‘natural language processing’, and ‘machine learning’. The literature review shows that the existing techniques and methods for performing sentiment analysis are not very accurate. Therefore, this study proposes a hybrid method for performing sentiment analysis, where the features of the text are extracted using TF</p>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# contrastive search\n",
        "output = model.generate(prompt, top_k=5, top_p=0.95, max_new_tokens=150)\n",
        "display_markdown(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b6aa2bc",
      "metadata": {
        "id": "0b6aa2bc"
      },
      "source": [
        "You can find more options in the Huggingface Transformers Documentation: https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig\n",
        "\n",
        "If you are interested in the topic Decoding and Sampling you can read further in the blogpost by von Platen on huggingface: https://huggingface.co/blog/how-to-generate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70f45bb3",
      "metadata": {
        "id": "70f45bb3"
      },
      "source": [
        "## 10. Composition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97a499b9",
      "metadata": {
        "id": "97a499b9"
      },
      "source": [
        "Galactica models are able to mix & combine scientific modalities, stored knowledge and generalize to new tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "6271bf6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6271bf6b",
        "outputId": "e154f4c1-cd89-44d2-e3cb-a773a1691836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Translate the following python code:\n",
            "\n",
            "```python\n",
            "def cheapestProduct(products: List[Product]) -> Product:\n",
            "    return min(products, key=lambda p: p.price)\n",
            "```\n",
            "\n",
            "into C++.\n",
            "\n",
            "Answer:\n",
            "\n",
            "```\n",
            "#include <iostream>\n",
            "#include <cstdlib>\n",
            "#include <cstring>\n",
            "#include <cmath>\n",
            "#include <cstdio>\n",
            "#include <cmath>\n",
            "#include <cstring>\n",
            "#include <cmath>\n",
            "#include <cstdio>\n",
            "#include <cmath>\n",
            "#include <cstdio>\n",
            "#include <cmath>\n",
            "#include <cstdio>\n",
            "#include <cmath>\n",
            "#include <cstdio>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "output = model.generate(\"\"\"Question: Translate the following python code:\n",
        "\n",
        "```python\n",
        "def cheapestProduct(products: List[Product]) -> Product:\n",
        "    return min(products, key=lambda p: p.price)\n",
        "```\n",
        "\n",
        "into C++.\n",
        "\n",
        "Answer:\"\"\", max_new_tokens=130)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "9f90d657",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f90d657",
        "outputId": "b17a4326-3b1d-48aa-c038-ee116f740750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Translate the following math formula:\n",
            "\n",
            "\\[\n",
            "  \\zeta(s) = \\sum_{n=1}^{\\infty} n^{-s}\n",
            "\\]\n",
            "\n",
            "into plain English.\n",
            "\n",
            "Answer: \\(\\zeta(s)=\\sum_{n=1}^{\\infty} n^{-s}=\\sum_{n=1}^{\\infty} \\frac{1}{n^{s}}\\)\n",
            "\n",
            "## Exercise \\(\\PageIndex{1}\\)\n",
            "\n",
            "Translate the following math formula:\n",
            "\n",
            "\\[\n",
            "  \\zeta(s) = \\sum_{n=\n"
          ]
        }
      ],
      "source": [
        "output = model.generate(\"\"\"Question: Translate the following math formula:\n",
        "\n",
        "\\\\[\n",
        "  \\\\zeta(s) = \\\\sum_{n=1}^{\\\\infty} n^{-s}\n",
        "\\\\]\n",
        "\n",
        "into plain English.\n",
        "\n",
        "Answer:\"\"\", max_new_tokens=100)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fad61cd",
      "metadata": {
        "id": "5fad61cd"
      },
      "source": [
        "## 11. Few-Shot Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1b2c79e",
      "metadata": {
        "id": "f1b2c79e"
      },
      "source": [
        "As we did with the sentiment analysis, we can use examples to teach the model our desired result format before we ask for the actual answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "c5113ef4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "c5113ef4",
        "outputId": "7f4b1c10-8244-4979-9eac-9df1578db666"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p>Question: does \"kayak\" read the same backward as forward? Answer with code.</p>\n",
              "<p>Code:</p>\n",
              "<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">def</span> <span class=\"nf\">is_palindrome</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">):</span>\n",
              "    <span class=\"k\">return</span> <span class=\"n\">s</span> <span class=\"o\">==</span> <span class=\"n\">s</span><span class=\"p\">[::</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n",
              "</code></pre></div>\n",
              "\n",
              "<p>Answer: <code>is_palindrome(\"kayak\")</code>.</p>\n",
              "<p>Question: An $i$-th Peanut Butter number is given by the formula $pb_i = \\prod_{k=2}^{i} \\frac{1}{1-1/k}$. An $i$-th Jelly number is given by $J_i = \\sum_{k=2}^{i} pb_k$. What is the 6-th Jelly number? Answer with code.</p>\n",
              "<p>Code:</p>\n",
              "<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">def</span> <span class=\"nf\">pb_i</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">):</span>\n",
              "    <span class=\"k\">return</span> <span class=\"mi\">1</span><span class=\"o\">/</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"o\">/</span><span class=\"n\">i</span><span class=\"p\">)</span>\n",
              "</code></pre></div>\n",
              "\n",
              "<p>Answer: <code>pb_6(6)</code>.</p>\n",
              "<p>Question: The $i$-th Fibonacci number is given by the formula $F_i = F_{i-1} + F_{i-2}$. The $i$-th Lucas number is given by the formula $L_i = L_{i-1} + L_{i-2}$. What is the 10-th Fibonacci number? Answer with code.</p>\n",
              "<p>Code:</p>\n",
              "<p>```</p>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "display_markdown(model.generate(\"\"\"Question: does \"kayak\" read the same backward as forward? Answer with code.\n",
        "\n",
        "Code:\n",
        "\n",
        "```python\n",
        "def is_palindrome(s):\n",
        "    return s == s[::-1]\n",
        "```\n",
        "\n",
        "Answer: `is_palindrome(\"kayak\")`.\n",
        "\n",
        "Question: An $i$-th Peanut Butter number is given by the formula $pb_i = \\\\prod_{k=2}^{i} \\\\frac{1}{1-1/k}$. An $i$-th Jelly number is given by $J_i = \\\\sum_{k=2}^{i} pb_k$. What is the 6-th Jelly number? Answer with code.\n",
        "\"\"\", max_new_tokens=150))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f822f59",
      "metadata": {
        "id": "9f822f59"
      },
      "source": [
        "#### TASK 3.4\n",
        "\n",
        "Use Few-Shot Prompting to classify the following paper based on its abstract into category.\n",
        "\n",
        "Hu et al. 2021, \"LoRA: Low-Rank Adaptation of Large Language Models\" (https://arxiv.org/abs/2106.09685)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22a00243",
      "metadata": {
        "id": "22a00243"
      },
      "source": [
        "Abstract:\n",
        "> An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at this https URL (https://github.com/microsoft/LoRA)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "6b3770da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "6b3770da",
        "outputId": "1c912358-dcdc-42df-e2f5-98aad0e01661"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p>Question: How would you classify the sentence \"Machine Learning is cool\"? Answer with code.</p>\n",
              "<p>Code:</p>\n",
              "<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">def</span> <span class=\"nf\">classify</span><span class=\"p\">(</span><span class=\"n\">sentence</span><span class=\"p\">):</span>\n",
              "\n",
              "    <span class=\"n\">examples</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n",
              "        <span class=\"p\">(</span><span class=\"s2\">&quot;This paper discusses an efficient approach to train large-scale deep learning models using model pruning, quantization, and distillation techniques, which reduce the computational cost and memory usage of the models without sacrificing performance.&quot;</span><span class=\"p\">,</span>\n",
              "         <span class=\"s2\">&quot;Machine Learning/Artificial Intelligence - Model Efficiency and Adaptation&quot;</span><span class=\"p\">),</span>\n",
              "\n",
              "        <span class=\"p\">(</span><span class=\"s2\">&quot;We propose a new method for named entity recognition in texts that uses a hybrid of rule-based and deep learning models to improve the extraction of entities such as names, dates, and locations.&quot;</span><span class=\"p\">,</span>\n",
              "         <span class=\"s2\">&quot;Natural Language Processing&quot;</span><span class=\"p\">),</span>\n",
              "\n",
              "        <span class=\"p\">(</span><span class=\"s2\">&quot;In this paper, we introduce a novel deep convolutional neural network architecture for image classification that achieves state-of-the-art performance on the CIFAR-10 dataset.&quot;</span><span class=\"p\">,</span>\n",
              "         <span class=\"s2\">&quot;Computer Vision&quot;</span><span class=\"p\">),</span>\n",
              "\n",
              "        <span class=\"p\">(</span><span class=\"s2\">&quot;We explore the use of deep reinforcement learning for game-playing agents, focusing on training an agent to play Atari games using Q-learning with a deep neural network.&quot;</span><span class=\"p\">,</span>\n",
              "         <span class=\"s2\">&quot;Reinforcement Learning&quot;</span><span class=\"p\">)</span>\n",
              "    <span class=\"p\">]</span>\n",
              "\n",
              "    <span class=\"k\">for</span> <span class=\"n\">example</span><span class=\"p\">,</span> <span class=\"n\">category</span> <span class=\"ow\">in</span> <span class=\"n\">examples</span><span class=\"p\">:</span>\n",
              "        <span class=\"k\">if</span> <span class=\"n\">example</span> <span class=\"ow\">in</span> <span class=\"n\">sentence</span><span class=\"p\">:</span>\n",
              "            <span class=\"k\">return</span> <span class=\"n\">category</span>\n",
              "\n",
              "    <span class=\"k\">return</span> <span class=\"s2\">&quot;Unknown Category&quot;</span>\n",
              "</code></pre></div>\n",
              "\n",
              "<p>Answer: <code>classify(\"Machine Learning is cool\")</code>.</p>\n",
              "<p>Question: How would you classify this paper based on its abstract; An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at this https URL (https://github.com/microsoft/LoRA)? Answer with code.</p>\n",
              "<p>Code:</p>\n",
              "<p>```python\n",
              "def classify_abstract(abstract):</p>\n",
              "<div class=\"codehilite\"><pre><span></span><code><span class=\"nv\">examples</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span>[\n",
              "<span class=\"w\">    </span><span class=\"ss\">(</span><span class=\"s2\">&quot;This paper discusses an efficient approach to train large-scale deep learning models using model pruning, quantization, and distillation techniques, which reduce the computational cost and memory usage of the models without sacrificing performance.&quot;</span>,\n",
              "<span class=\"w\">     </span><span class=\"s2\">&quot;Machine Learning/Artificial Intelligence - Model Efficiency and Adaptation&quot;</span><span class=\"ss\">)</span>,\n",
              "\n",
              "<span class=\"w\">    </span><span class=\"ss\">(</span><span class=\"s2\">&quot;We propose a new method for named entity recognition in texts that uses a hybrid of rule-based and deep learning models to improve the extraction of entities such as names, dates, and locations.&quot;</span>,\n",
              "<span class=\"w\">     </span><span class=\"s2\">&quot;Natural Language Processing&quot;</span><span class=\"ss\">)</span>,\n",
              "\n",
              "<span class=\"w\">    </span><span class=\"ss\">(</span><span class=\"err\">&quot;In</span>\n",
              "</code></pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "### IMPLEMENT YOUR SOLUTION HERE ###\n",
        "display_markdown(model.generate(\"\"\"Question: How would you classify the sentence \"Machine Learning is cool\"? Answer with code.\n",
        "\n",
        "Code:\n",
        "\n",
        "```python\n",
        "def classify(sentence):\n",
        "\n",
        "    examples = [\n",
        "        (\"This paper discusses an efficient approach to train large-scale deep learning models using model pruning, quantization, and distillation techniques, which reduce the computational cost and memory usage of the models without sacrificing performance.\",\n",
        "         \"Machine Learning/Artificial Intelligence - Model Efficiency and Adaptation\"),\n",
        "\n",
        "        (\"We propose a new method for named entity recognition in texts that uses a hybrid of rule-based and deep learning models to improve the extraction of entities such as names, dates, and locations.\",\n",
        "         \"Natural Language Processing\"),\n",
        "\n",
        "        (\"In this paper, we introduce a novel deep convolutional neural network architecture for image classification that achieves state-of-the-art performance on the CIFAR-10 dataset.\",\n",
        "         \"Computer Vision\"),\n",
        "\n",
        "        (\"We explore the use of deep reinforcement learning for game-playing agents, focusing on training an agent to play Atari games using Q-learning with a deep neural network.\",\n",
        "         \"Reinforcement Learning\")\n",
        "    ]\n",
        "\n",
        "    for example, category in examples:\n",
        "        if example in sentence:\n",
        "            return category\n",
        "\n",
        "    return \"Unknown Category\"\n",
        "\n",
        "```\n",
        "Answer: `classify(\"Machine Learning is cool\")`.\n",
        "\n",
        "Question: How would you classify this paper based on its abstract; An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at this https URL (https://github.com/microsoft/LoRA)? Answer with code.\n",
        "\"\"\", max_new_tokens=150))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cf601f0",
      "metadata": {
        "id": "3cf601f0"
      },
      "source": [
        "## 12. Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b215fc5",
      "metadata": {
        "id": "0b215fc5"
      },
      "source": [
        "\n",
        "As we observed already, we have different tokens in the galactica models that help to structure the knowledge and tasks. All Galactica models share the same vocabulary of 50000 tokens. The vocabulary was trained on 2% of our training corpus using Byte-Pair Encoding (BPE) tokenization.\n",
        "\n",
        "The following provides a overview on the tokens and what makes galactica special.\n",
        "\n",
        "### Special Tokens\n",
        "\n",
        "Some of the tokens (f.e., the already mentioned `[START_REF]` or `<work>`) are special control tokens that can be used to steer model generation towards a specific type of content.\n",
        "\n",
        "\n",
        "`<unk>` - reserved.\n",
        "\n",
        "`<s>` - reserved.\n",
        "\n",
        "`</s>` - end-of-document token used to split documents during trainig. Prepending this token to prompt (see `new_doc` parameter in `Model.generate`) biases a model into generating a new document.\n",
        "\n",
        "`<pad>` - a standard padding token to align sequences in a batch.\n",
        "\n",
        "`[START_REF]` and `[END_REF]` - markers denoting a reference to a paper. Each paper is represented as `Title, First author name`. F.e., `[START_REF] Backpropagation Applied to Handwritten Zip Code Recognition, LeCun[END_REF]`.\n",
        "\n",
        "`[IMAGE]` - a placeholder for an image removed from a text.\n",
        "\n",
        "`<fragments>` and `</fragments>` - markers denoting fragments in FragmentedGlass dataset.\n",
        "\n",
        "`<work>` and `</work>` - markers denoting step-by-step reasoning (see Step-by-Step Reasoning Section).\n",
        "\n",
        "`[START_SUP]`, `[END_SUP]`, `[START_SUB]` and `[END_SUB]` - markers used to protect superscript and subscript digits from NFKC normaliziation. Our tokenizer uses the standard NFKC rules, which means that `x²⁵` would be tokenized in the same way as `x25`. To prevent this, we encode `x²⁵` as `x[START_SUP]25[END_SUP]`.\n",
        "\n",
        "`[START_DNA]`, `[END_DNA]`, `[START_AMINO]`, `[END_AMINO]`, `[START_SMILES]`, `[END_SMILES]`, `[START_I_SMILES]` and `[END_I_SMILES]` - markers denoting special sequences, respectively: nucleic acids sequences, amino acids sequeqnces, canonical simplified molecular-input line-entry system (SMILES) strings and isometric SMILES strings. Besides marking a sequence of a given type, these tokens force a special tokenization mode in which each character is represented as a single token. F.e., `GATTACA` is tokenized as `G|ATT|ACA`, while `[START_DNA]GATTACA[END_DNA]` is tokenized as `[START_DNA]|G|A|T|T|A|C|A|[END_DNA]`. Note that for this to work you need to transform your prompt with `galai.utils.escape_custom_split_sequence`. All standard text generation functions of `galai.model.Model` do this automatically.\n",
        "\n",
        "The `galai` library takes care of handling of the special tokens. If you are using `tokenizers` directly then most likely you want to keep the special tokens in the output for further processing. Set `skip_special_tokens=False` in `tokenizers.Tokenizer.decode`.\n",
        "\n",
        "### Decoupling of Tokens\n",
        "\n",
        "The BPE training algorithm creates vocabulary based on frequncies of subwords in the training corpus, with more frequent subwords being represented with fewer number of tokens. This means that visually similar subwords may end up having totally different token representations. For example, in the GPT-2 tokenizer (trained before year 2020) each of the numbers `{2000, 2001, ..., 2020}` is encoded with a unique token, and all of the numbers `{2021, 2022, ..., 2030}` are represented as two tokens: `20|21`, `20|22`, etc. Training on a corpus with math, TeX formulas and source code it can happen that a single token encodes multiple independent functions. F.e., `\\(-` can end up being a single token making prompting more difficult and the model less robust to changes in spaces.\n",
        "\n",
        "To prevent this issue galactica implements custom splitting rules, presented in the example below. For performance reasons we keep a leading space (i.e., ` text` can be a single token)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "1c927d4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "1c927d4d",
        "outputId": "a99cd0a1-adee-466b-c622-7c47bad35a20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.tok-examp1e > span {border: 1px solid #555; padding: 4px 6px; margin: 2px; background: #f8f8f8}</style><div class='tok-examp1e' style='display: flex; flex-wrap: wrap'><span>Token</span><span>ization</span><span>▁of</span><span>▁most</span><span>▁of</span><span>▁the</span><span>▁natural</span><span>▁texts</span><span>▁is</span><span>▁not</span><span>▁impacted</span><span>▁by</span><span>▁the</span><span>▁rules</span><span>.</span><span>\\n</span><span>However</span><span>,</span><span>▁most</span><span>▁of</span><span>▁the</span><span>▁non</span><span>-</span><span>al</span><span>phan</span><span>um</span><span>eric</span><span>▁ASC</span><span>II</span><span>▁characters</span><span>▁are</span><span>▁split</span><span>.</span><span>▁This</span><span>▁is</span><span>▁mostly</span><span>▁visible</span><span>▁in</span><span>▁Te</span><span>X</span><span>▁formulas</span><span>,</span><span>\\n</span><span>for</span><span>▁example</span><span>:</span><span>▁</span><span>$</span><span>\\</span><span>frac</span><span>{</span><span>d</span><span>}</span><span>{</span><span>dx</span><span>}</span><span>\\</span><span>,</span><span>\\</span><span>cos</span><span>(</span><span>x</span><span>)</span><span>▁</span><span>=</span><span>▁</span><span>-</span><span>\\</span><span>sin</span><span>(</span><span>x</span><span>)</span><span>$</span><span>,</span><span>▁</span><span>\\</span><span>(</span><span>\\</span><span>zeta</span><span>(</span><span>s</span><span>)</span><span>=</span><span>\\</span><span>sum</span><span>_</span><span>{</span><span>n</span><span>=</span><span>1</span><span>}</span><span>^</span><span>{</span><span>\\</span><span>infty</span><span>}</span><span>▁n</span><span>^</span><span>{</span><span>-</span><span>s</span><span>}</span><span>\\</span><span>)</span><span>.</span><span>▁</span><span>\\n</span><span>It</span><span>▁also</span><span>▁impacts</span><span>▁source</span><span>▁codes</span><span>,</span><span>▁like</span><span>:</span><span>▁x</span><span>+</span><span>=</span><span>(</span><span>(</span><span>1</span><span>,</span><span>2</span><span>)</span><span>)</span><span>;</span><span>▁</span><span>\\n</span><span>As</span><span>▁a</span><span>▁side</span><span>-</span><span>effect</span><span>,</span><span>▁contractions</span><span>▁</span><span>(</span><span>I</span><span>&#x27;</span><span>ll</span><span>,</span><span>▁you</span><span>&#x27;</span><span>ve</span><span>,</span><span>▁it</span><span>&#x27;</span><span>s</span><span>,</span><span>▁etc</span><span>.</span><span>)</span><span>▁and</span><span>▁em</span><span>otic</span><span>ons</span><span>▁</span><span>(</span><span>like</span><span>▁this</span><span>▁Santa</span><span>▁Claus</span><span>▁</span><span>*</span><span>&lt;</span><span>|</span><span>:</span><span>‐</span><span>)</span><span>▁</span><span>)</span><span>▁are</span><span>▁split</span><span>.</span><span>▁</span><span>\\n</span><span>This</span><span>▁rule</span><span>▁makes</span><span>▁exception</span><span>▁for</span><span>▁a</span><span>▁repeated</span><span>▁sequence</span><span>▁of</span><span>▁the</span><span>▁same</span><span>▁character</span><span>,</span><span>▁so</span><span>▁f</span><span>.</span><span>e</span><span>.</span><span>,</span><span>▁</span><span>----------------</span><span>▁is</span><span>▁still</span><span>▁a</span><span>▁single</span><span>▁token</span><span>.</span><span>▁</span><span>\\n</span><span>Additionally</span><span>,</span><span>▁E</span><span>OL</span><span>▁character</span><span>▁is</span><span>▁always</span><span>▁split</span><span>,</span><span>▁so</span><span>▁that</span><span>▁</span><span>\\n</span><span>\\n</span><span>\\n</span><span>\\n</span><span>\\n</span><span>are</span><span>▁</span><span>5</span><span>▁tokens</span><span>.</span><span>▁</span><span>\\n</span><span>Num</span><span>bers</span><span>▁are</span><span>▁slit</span><span>▁into</span><span>▁individual</span><span>▁digits</span><span>▁as</span><span>▁before</span><span>,</span><span>▁f</span><span>.</span><span>e</span><span>.</span><span>,</span><span>▁</span><span>$$</span><span>\\</span><span>pi</span><span>=</span><span>3</span><span>.</span><span>1</span><span>4</span><span>1</span><span>5</span><span>9</span><span>2</span><span>6</span><span>5</span><span>\\</span><span>ldots</span><span>$$</span><span>▁</span><span>\\n</span><span>Note</span><span>▁that</span><span>▁non</span><span>-</span><span>al</span><span>phan</span><span>um</span><span>eric</span><span>▁splitting</span><span>▁splits</span><span>▁space</span><span>▁in</span><span>▁front</span><span>▁as</span><span>▁well</span><span>▁</span><span>(</span><span>f</span><span>.</span><span>e</span><span>.</span><span>,</span><span>▁i</span><span>▁</span><span>++</span><span>,</span><span>▁x</span><span>▁</span><span>&lt;</span><span>-</span><span>&gt;</span><span>▁y</span><span>,</span><span>▁if</span><span>▁</span><span>(</span><span>▁x</span><span>▁</span><span>&lt;</span><span>=</span><span>▁y</span><span>▁</span><span>)</span><span>)</span><span>.</span><span>▁</span><span>\\n</span><span>Special</span><span>▁tokens</span><span>▁like</span><span>▁</span><span>[START_REF]</span><span>,</span><span>▁</span><span>&lt;work&gt;</span><span>▁or</span><span>▁</span><span>[IMAGE]</span><span>▁are</span><span>▁left</span><span>▁intact</span><span>.</span><span>▁</span><span>\\n</span><span>The</span><span>▁tokenizer</span><span>▁additionally</span><span>▁supports</span><span>▁custom</span><span>▁sequence</span><span>▁splitting</span><span>▁</span><span>(</span><span>does</span><span>▁not</span><span>▁work</span><span>▁by</span><span>▁default</span><span>,</span><span>▁requires</span><span>▁a</span><span>▁custom</span><span>▁preprocessing</span><span>▁step</span><span>)</span><span>,</span><span>▁f</span><span>.</span><span>e</span><span>.</span><span>:</span><span>▁</span><span>\\n</span><span>[START_DNA]</span><span>G</span><span>A</span><span>T</span><span>T</span><span>A</span><span>C</span><span>A</span><span>[END_DNA]</span><span>,</span><span>▁</span><span>[START_AMINO]</span><span>P</span><span>E</span><span>P</span><span>T</span><span>I</span><span>D</span><span>E</span><span>S</span><span>[END_AMINO]</span><span>,</span><span>▁</span><span>\\n</span><span>[START_SMILES]</span><span>C</span><span>C</span><span>(</span><span>=</span><span>O</span><span>)</span><span>N</span><span>C</span><span>C</span><span>C</span><span>1</span><span>=</span><span>C</span><span>N</span><span>c</span><span>2</span><span>c</span><span>1</span><span>c</span><span>c</span><span>(</span><span>O</span><span>C</span><span>)</span><span>c</span><span>c</span><span>2</span><span>[END_SMILES]</span><span>▁and</span><span>▁</span><span>[START_I_SMILES]</span><span>C</span><span>N</span><span>1</span><span>C</span><span>C</span><span>C</span><span>[</span><span>C</span><span>@</span><span>H</span><span>]</span><span>1</span><span>c</span><span>2</span><span>c</span><span>c</span><span>c</span><span>n</span><span>c</span><span>2</span><span>[END_I_SMILES]</span></div>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "from galai.utils import escape_custom_split_sequence\n",
        "from IPython.display import HTML\n",
        "import html\n",
        "\n",
        "def tokenization_example(tokenizer, text):\n",
        "    text = escape_custom_split_sequence(text)\n",
        "    tokens = [tokenizer.decode([x], skip_special_tokens=False) for x in tokenizer.encode(text)]\n",
        "    spans = \"</span><span>\".join([html.escape(t).replace(\" \", \"▁\").replace(\"\\n\", \"\\\\n\") for t in tokens])\n",
        "    style = \"<style>.tok-examp1e > span {border: 1px solid #555; padding: 4px 6px; margin: 2px; background: #f8f8f8}</style>\"\n",
        "    return HTML(style + \"<div class='tok-examp1e' style='display: flex; flex-wrap: wrap'><span>\" + spans + \"</span></div>\")\n",
        "\n",
        "tokenization_example(model.tokenizer, r\"\"\"Tokenization of most of the natural texts is not impacted by the rules.\n",
        "However, most of the non-alphanumeric ASCII characters are split. This is mostly visible in TeX formulas,\n",
        "for example: $\\frac{d}{dx}\\,\\cos(x) = -\\sin(x)$, \\(\\zeta(s)=\\sum_{n=1}^{\\infty} n^{-s}\\).\n",
        "It also impacts source codes, like: x+=((1,2));\n",
        "As a side-effect, contractions (I'll, you've, it's, etc.) and emoticons (like this Santa Claus *<|:‑) ) are split.\n",
        "This rule makes exception for a repeated sequence of the same character, so f.e., ---------------- is still a single token.\n",
        "Additionally, EOL character is always split, so that\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "are 5 tokens.\n",
        "Numbers are slit into individual digits as before, f.e., $$\\pi=3.14159265\\ldots$$\n",
        "Note that non-alphanumeric splitting splits space in front as well (f.e., i ++, x <-> y, if ( x <= y )).\n",
        "Special tokens like [START_REF], <work> or [IMAGE] are left intact.\n",
        "The tokenizer additionally supports custom sequence splitting (does not work by default, requires a custom preprocessing step), f.e.:\n",
        "[START_DNA]GATTACA[END_DNA], [START_AMINO]PEPTIDES[END_AMINO],\n",
        "[START_SMILES]CC(=O)NCCC1=CNc2c1cc(OC)cc2[END_SMILES] and [START_I_SMILES]CN1CCC[C@H]1c2cccnc2[END_I_SMILES]\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50003559",
      "metadata": {
        "id": "50003559"
      },
      "source": [
        "## 13. Limitations and Pitfalls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0600a19a",
      "metadata": {
        "id": "0600a19a"
      },
      "source": [
        "While Galactica language models enable one to analyze and work with scientific data in multiple new ways, it's important to understand the shortcomings of the models. To explore the models limitations we explore some in the following.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d0cefd1",
      "metadata": {
        "id": "6d0cefd1"
      },
      "source": [
        "### Hallucinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "93c0b959",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93c0b959",
        "outputId": "2c946c01-da52-4411-eb59-2c8659e8583b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Götz-Henrik Wiegand\n",
            "\n",
            " Götz-Henrik Wiegand (born 1960) is a German politician of the Christian Democratic Union (CDU) who has been serving as a member of the Bundestag from the state of Hesse since 2017.\n",
            "\n",
            "## Political career\n",
            "\n",
            " Wiegand was elected to the Bundestag in the 2017 German federal election. In parliament, he serves on the Committee on Foreign Affairs.\n",
            "\n",
            "## External links\n",
            "\n",
            "* Official website (in German)\n",
            "* Bundestag biography (\n"
          ]
        }
      ],
      "source": [
        "print(model.generate(\"# Götz-Henrik Wiegand\\n\", max_new_tokens=120))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34ac4278",
      "metadata": {
        "id": "34ac4278"
      },
      "source": [
        "The issue is especially visible in case of prompts with incorrect assumptions, in which a prompt already includes made up statements. Here are some examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "493f6087",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "493f6087",
        "outputId": "e56b7ef9-2376-44b4-b8f7-c6d93a199e0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The Einstein-Handschuh-Wiegand equation is given by:\n",
              "\n",
              "$$ \\displaystyle\\frac{d^{2}\\mathbf{r}}{dt^{2}}=\\frac{\\mathbf{F}}{m} $$ (1)\n",
              "\n",
              "where $\\mathbf{r}$ is the position vector of the particle, $m$ is the mass of the particle, and $\\mathbf{F}$ is the force acting on the particle.\n",
              "\n",
              "The force acting on a particle is given by:\n",
              "\n",
              "$$ \\displaystyle\\mathbf{F}=\\mathbf{F}_{\\text{ext}}+\\mathbf{F}_{\\text{int}} $$ (2)\n",
              "\n",
              "where $\\mathbf{F}_{\\text{ext}}$ is the external force and $\\mathbf{F}_{\\text{int}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "display_latex(\n",
        "    model.generate(\n",
        "        \"The Einstein-Handschuh-Wiegand equation is given by:\\n\",\n",
        "        max_new_tokens=200,\n",
        "        new_doc=True\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "7b2d09c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b2d09c2",
        "outputId": "8a9d8734-ecd9-4e7d-9f3d-53822f4f22c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: what was the main reason that lead to the duel between Yann LeCun and Jürgen Schmidhuber?\n",
            "\n",
            "Answer: Schmidhuber's work was more focused on the development of a general purpose learning algorithm, while LeCun's work was more focused on the development of a specific learning algorithm.\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    model.generate(\n",
        "        \"Question: what was the main reason that lead to the duel between Yann LeCun and Jürgen Schmidhuber?\\n\\nAnswer:\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "4029dd7a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4029dd7a",
        "outputId": "0271de2a-2bbb-46e0-969c-1abf8166b69e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: what is the largest prime number?\n",
            "\n",
            "Answer: $2^{2^{2^{2^{2^{2^{2^{2^{2^{2^{2^{2^{2^{2^{2^{2^{2^{2^{2^{2\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    model.generate(\n",
        "        \"Question: what is the largest prime number?\\n\\nAnswer:\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c940915",
      "metadata": {
        "id": "0c940915"
      },
      "source": [
        "### Multi Lingual"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "816bb908",
      "metadata": {
        "id": "816bb908"
      },
      "source": [
        "The Galactica models are not multi-lingual by design. Most of the natural language documents in the NatureBook corpus are written in **English**. Prompting in different language results in more random generations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "985835da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "985835da",
        "outputId": "d3f2b417-6e08-4193-df89-4122603bb5ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " # Galaxie\n",
            "Eine Galaxie ist eine Ansammlung von Sternen, die sich in einem einzigen, einzigen, einzigen, einzigen, einzigen, einzigen, einzigen, einzigen, einzigen, einzigen, einzigen, einzigen, einzigen, einzigen, einz\n"
          ]
        }
      ],
      "source": [
        "print(model.generate(\" # Galaxie\\nEine Galaxie ist eine Ansammlung von Sternen,\", new_doc=True, max_new_tokens=65))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a865e511",
      "metadata": {
        "id": "a865e511"
      },
      "source": [
        "A translation by a native speaker:\n",
        "> A galaxy is a group of stars, galaxies, planetary systems, etc. that are located in a specific region of the universe.\n",
        "Galaxy is a tool to generate galaxy simulations at a specific time of the Universe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "e4c8724b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4c8724b",
        "outputId": "e3b38c8b-baa5-4f38-d3ff-0e6ebd24eb43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: how do you say 'Good morning' in German?\n",
            "\n",
            "Answer: Good morning\n"
          ]
        }
      ],
      "source": [
        "print(model.generate(\"Question: how do you say 'Good morning' in German?\\n\\nAnswer:\", max_new_tokens=65)) # correct would be \"Guten Morgen\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "f1af61eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1af61eb",
        "outputId": "bfc1efea-00be-436d-a3f1-86f23f150ef9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: how do you say 'Good morning' in Italian?\n",
            "\n",
            "Answer: 'Già'\n"
          ]
        }
      ],
      "source": [
        "print(model.generate(\"Question: how do you say 'Good morning' in Italian?\\n\\nAnswer:\", max_new_tokens=65)) # correct would be \"Buongiorno\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b00413a",
      "metadata": {
        "id": "8b00413a"
      },
      "source": [
        "The NatureBook corpus was assembled in July 2022, so the models have no information about anything that happened after."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "be4a95f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be4a95f5",
        "outputId": "7e46d086-f32e-4816-dbe0-7c5957233355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Elizabeth II\n",
            "\n",
            " Elizabeth II may refer to:\n",
            "\n",
            "* Elizabeth II (1558–1618), Queen of England and Scotland\n",
            "* Elizabeth II (1618–1658), Queen of England and Scotland\n",
            "* Elizabeth II (16\n"
          ]
        }
      ],
      "source": [
        "print(model.generate(\"# Elizabeth II\\n\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "3c1c748b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c1c748b",
        "outputId": "55e5e161-0d62-4b44-c573-b966aea1a1ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What year is it?\n",
            "\n",
            "Answer: 1999\n",
            "\n",
            "Question: What is the year of the year?\n",
            "\n",
            "Answer: 1999\n",
            "\n",
            "Question: What is the year of the year?\n",
            "\n",
            "Answer: 1999\n",
            "\n",
            "Question: What is the year of the year?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(model.generate(\"Question: What year is it?\\n\\nAnswer:\", new_doc=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690edf0d",
      "metadata": {
        "id": "690edf0d"
      },
      "source": [
        "### Prompt Robustness\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c38c8c74",
      "metadata": {
        "id": "c38c8c74"
      },
      "source": [
        "#### Spelling Errors\n",
        "A large part of the NatureBook corpus consists of documents using a formal and technical language. The model output may change depending on spelling, punctuation and grammatical errors in a prompt.\n",
        "\n",
        "**Reality**: Jürgen Schmidhuber is born 1963 and studied at Technischen Universität München"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "72807d63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72807d63",
        "outputId": "072d251d-e463-45bd-ede0-9164dedce975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Jürgen Schmithuber\n",
            "\n",
            " Jürgen Schmithuber (born 1953) is a German politician of the Christian Democratic Union (CDU) who has been serving as a member of the Bundestag from the state of North Rhine-Westphalia since 2005.\n"
          ]
        }
      ],
      "source": [
        "print(model.generate(\"# Jürgen Schmithuber\\n\"))  # a typo in the last name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "9770a4f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9770a4f4",
        "outputId": "83dfaac8-a2e0-4102-d5c7-46734127d3a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Jürgen Schmidhuber\n",
            "\n",
            " Jürgen Schmidhuber (born 1950) is a German politician of the Christian Democratic Union (CDU) who has been serving as a member of the Bundestag from the state of North Rhine-Westphalia since 2005.\n"
          ]
        }
      ],
      "source": [
        "print(model.generate(\"# Jürgen Schmidhuber\\n\")) # correct name"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7c988b4",
      "metadata": {
        "id": "e7c988b4"
      },
      "source": [
        "**Note:** Depending on the model size, we still might get some wrong results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b023307",
      "metadata": {
        "id": "5b023307"
      },
      "source": [
        "#### TeX formula markers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb5636c4",
      "metadata": {
        "id": "eb5636c4"
      },
      "source": [
        "Most of the documents in the NatureBook corpus use `\\(` and `\\)` for inline TeX formulas and `\\[` and `\\]` for display mode maths, but some of the data sources use `$` and `$$` instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "6cd9da4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "6cd9da4d",
        "outputId": "2a69054d-feaa-4002-be8a-b0a80f21d940"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p>Question: What is the expected value of a random variable uniformly distributed over the interval $[a^2, b+c]$?</p>\n",
              "<p>Answer: $\\dfrac{b+c-a^2}{b+c}\\</p>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "# using \\( \\)\n",
        "display_markdown(\n",
        "    model.generate(\n",
        "\"\"\"Question: What is the expected value of a random variable uniformly distributed over the interval \\\\([a^2, b+c]\\\\)?\n",
        "\n",
        "Answer:\"\"\", max_new_tokens=20)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "3c01248f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "3c01248f",
        "outputId": "7baf4760-b69b-4a80-8e89-16a2761aeb0d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p>Question: What is the expected value of a random variable uniformly distributed over the interval $$[a^2, b+c]$$?</p>\n",
              "<p>Answer: $$\\dfrac{b+c-a^2}{b+c}\\</p>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# using \\[ \\]\n",
        "display_markdown(\n",
        "    model.generate(\n",
        "\"\"\"Question: What is the expected value of a random variable uniformly distributed over the interval \\\\[[a^2, b+c]\\\\]?\n",
        "\n",
        "Answer:\"\"\", max_new_tokens=20)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "b9851e04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "b9851e04",
        "outputId": "e3e8f026-cf99-40f8-d87c-bf2f48207bcb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p>Question: What is the expected value of a random variable uniformly distributed over the interval $[a^2, b+c]$?</p>\n",
              "<p>Answer: $\\frac{b+c-a^2}{b+c}$</p>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "# using $\n",
        "display_markdown(\n",
        "    model.generate(\n",
        "\"\"\"Question: What is the expected value of a random variable uniformly distributed over the interval $[a^2, b+c]$?\n",
        "\n",
        "Answer:\"\"\", max_new_tokens=500)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "65d48741",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "65d48741",
        "outputId": "d3aa2c38-b250-4317-bd44-ce59e6b9d379"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p>Question: What is the expected value of a random variable uniformly distributed over the interval $$[a^2, b+c]$$?</p>\n",
              "<p>Answer: $$\\frac{b+c-a^2}{b+c}$$</p>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "# using $$\n",
        "display_markdown(\n",
        "    model.generate(\n",
        "\"\"\"Question: What is the expected value of a random variable uniformly distributed over the interval $$[a^2, b+c]$$?\n",
        "\n",
        "Answer:\"\"\", max_new_tokens=40)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "3334951a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "3334951a",
        "outputId": "02d63d2b-a5d9-40a2-84b4-0b2eea206cd0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p>Question: What is the expected value of a random variable uniformly distributed over the interval [a^2, b+c]?</p>\n",
              "<p>Answer: $\\frac{b+c-a^2}{b+c}$</p>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "# plaintext math\n",
        "display_markdown(\n",
        "    model.generate(\n",
        "\"\"\"Question: What is the expected value of a random variable uniformly distributed over the interval [a^2, b+c]?\n",
        "\n",
        "Answer:\"\"\", max_new_tokens=22)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "ab568c51",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ab568c51",
        "outputId": "2fe57114-e7ac-419a-f032-83783a8c9ec8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p>Question: What is the expected value of a random variable uniformly distributed over the interval $[a^2, b+c]$?</p>\n",
              "<p>Answer: $\\frac{b+c-a^2}{b+c}$</p>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "# using $, beam search\n",
        "display_markdown(\n",
        "    model.generate(\n",
        "\"\"\"Question: What is the expected value of a random variable uniformly distributed over the interval $[a^2, b+c]$?\n",
        "\n",
        "Answer:\"\"\", max_new_tokens=50, num_beams=5)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba901a79",
      "metadata": {
        "id": "ba901a79"
      },
      "source": [
        "#### Letter-case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "3cc35113",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cc35113",
        "outputId": "e65b8354-8fc7-42f1-dcd7-df74d58c5b2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: what is Alzheimer's Disease?\n",
            "\n",
            "Alzheimer's Disease (AD) is a progressive neurodegenerative disorder that is the most common cause of dementia in the elderly. It is characterized by the presence of extracellular amyloid plaques and intracellular neurofibrillary tangles. The amyloid plaques are composed of amyloid-β (Aβ) peptides, which\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    model.generate(\"Question: what is Alzheimer's Disease?\\n\\n\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "38be3397",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38be3397",
        "outputId": "a7c93d1b-1388-421d-b215-bef106eeb55e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: what is alzheimer's disease?\n",
            "\n",
            "Answer: Alzheimer's disease (AD) is a progressive neurodegenerative disorder that is the most common cause of dementia in the elderly. It is characterized by the presence of extracellular amyloid plaques and intracellular neurofibrillary tangles. The amyloid plaques are composed of amyloid-β (Aβ) peptides,\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    model.generate(\"Question: what is alzheimer's disease?\\n\\n\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "de77a26a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de77a26a",
        "outputId": "84af7329-5212-4924-bd10-1c8f87fd9dc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: what is ALZHEIMER'S DISEASE?\n",
            "\n",
            "Answer: ALZHEIMER'S DISEASE is a rare, chronic, progressive, neurodegenerative disorder that affects the brain and spinal cord. It is characterized by progressive dementia, ataxia, and spasticity.\n",
            "\n",
            "ALZHEIMER'S DISEASE is a rare, chronic\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    model.generate(\"Question: what is ALZHEIMER'S DISEASE?\\n\\n\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b87116c",
      "metadata": {
        "id": "4b87116c"
      },
      "source": [
        "In addition to the problems outlined here, there are other problems. Some of these are listed in the paper (https://arxiv.org/abs/2211.09085)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b72774e8",
      "metadata": {
        "id": "b72774e8"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afbc5b83",
      "metadata": {
        "id": "afbc5b83"
      },
      "source": [
        "Galactica shows how, with clever training, elements of reasoning — though not on the level of modern models like OpenAI’s o-Models or DeepSeek — could be incorporated into LLMs as early as 2022."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp24_Solution",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f4dbc88916924c619d346ee2470ad469": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e22258979a774e2f9a99725b485b6696",
              "IPY_MODEL_93ea3f93c3c84a1b89750032d17213d5",
              "IPY_MODEL_e87b0e1818bc44c4bbaee9b53aee1f12"
            ],
            "layout": "IPY_MODEL_ece194e90bb241df8a8d2eb4f3117e65"
          }
        },
        "e22258979a774e2f9a99725b485b6696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6acf234eb484deebca421cdac357618",
            "placeholder": "​",
            "style": "IPY_MODEL_3db0b4153d144e13a7c1ff54d3c03573",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "93ea3f93c3c84a1b89750032d17213d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85fff5bc87554bfc991bf949fd34404d",
            "max": 166,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_906a26a77fbc4128a84f2482822b65fa",
            "value": 166
          }
        },
        "e87b0e1818bc44c4bbaee9b53aee1f12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2005d1f59dde4cdabae162fed2f11035",
            "placeholder": "​",
            "style": "IPY_MODEL_6b1a321f482d4b2984a1bcbc8ce4fe4d",
            "value": " 166/166 [00:00&lt;00:00, 2.69kB/s]"
          }
        },
        "ece194e90bb241df8a8d2eb4f3117e65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6acf234eb484deebca421cdac357618": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3db0b4153d144e13a7c1ff54d3c03573": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85fff5bc87554bfc991bf949fd34404d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "906a26a77fbc4128a84f2482822b65fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2005d1f59dde4cdabae162fed2f11035": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b1a321f482d4b2984a1bcbc8ce4fe4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63a63be020bd44d781088e40150b3547": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_19c680ba057744dbb8e9eac5b6e833df",
              "IPY_MODEL_cc3ac398e9fb4068a5ffb0b2679c3c75",
              "IPY_MODEL_ef52c5e249cd41698dd8c6ca45958bc8"
            ],
            "layout": "IPY_MODEL_a723c87ac48746bb8b65e51ed9046e06"
          }
        },
        "19c680ba057744dbb8e9eac5b6e833df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3683c2af1fff4db6bcab3fb37be5842f",
            "placeholder": "​",
            "style": "IPY_MODEL_54f1b44f47754f97b1accacd287051e1",
            "value": "tokenizer.json: 100%"
          }
        },
        "cc3ac398e9fb4068a5ffb0b2679c3c75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_642b00940706498fbcfe236045e9e453",
            "max": 2138869,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c957c5077e0d4b8988b052467736ea70",
            "value": 2138869
          }
        },
        "ef52c5e249cd41698dd8c6ca45958bc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ff2a567701a44749249a2ab7e2c015e",
            "placeholder": "​",
            "style": "IPY_MODEL_495b90022517498e8c8b859d903b68b3",
            "value": " 2.14M/2.14M [00:00&lt;00:00, 11.1MB/s]"
          }
        },
        "a723c87ac48746bb8b65e51ed9046e06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3683c2af1fff4db6bcab3fb37be5842f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54f1b44f47754f97b1accacd287051e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "642b00940706498fbcfe236045e9e453": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c957c5077e0d4b8988b052467736ea70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ff2a567701a44749249a2ab7e2c015e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "495b90022517498e8c8b859d903b68b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1b135c9bd834ae2b26746a447a36d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8c395a26ea64f468108d86e8ac2539f",
              "IPY_MODEL_316a4182f0244d499606c59c468109c2",
              "IPY_MODEL_75e4f2c1380a40ffac162d92b60d605b"
            ],
            "layout": "IPY_MODEL_784d251db7db42f28f191a4d244b484c"
          }
        },
        "e8c395a26ea64f468108d86e8ac2539f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43f9bfcecd35434fb66384841d00ca53",
            "placeholder": "​",
            "style": "IPY_MODEL_42fadf6cb06245be81135ba22a6b5731",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "316a4182f0244d499606c59c468109c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a7abe5f063d47feaf388252e2d5d7b7",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04933256654b4983a62e8eee9d3bdbf8",
            "value": 3
          }
        },
        "75e4f2c1380a40ffac162d92b60d605b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a25a338a6e48480bb0b62c90d0e3b487",
            "placeholder": "​",
            "style": "IPY_MODEL_4eb073de9e5e4b95a94b436853295834",
            "value": " 3.00/3.00 [00:00&lt;00:00, 99.9B/s]"
          }
        },
        "784d251db7db42f28f191a4d244b484c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43f9bfcecd35434fb66384841d00ca53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42fadf6cb06245be81135ba22a6b5731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a7abe5f063d47feaf388252e2d5d7b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04933256654b4983a62e8eee9d3bdbf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a25a338a6e48480bb0b62c90d0e3b487": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eb073de9e5e4b95a94b436853295834": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "054187455bb7443dbd6706318a42191e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_07c81e3bc2264037a78e6986a1daa55b",
              "IPY_MODEL_4f8271f93ff64572a20b8438dfc42582",
              "IPY_MODEL_abe386171f024ec2b129e55d57a48eef"
            ],
            "layout": "IPY_MODEL_3aee636048ca41dda07c9a55f39d0184"
          }
        },
        "07c81e3bc2264037a78e6986a1daa55b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5677475bde1e43a1936cfc18ce80e368",
            "placeholder": "​",
            "style": "IPY_MODEL_673d30aec9bb49918f2815d2fe37d1e2",
            "value": "config.json: 100%"
          }
        },
        "4f8271f93ff64572a20b8438dfc42582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_208d3bc2438345f8a2cc1f7acaa760b7",
            "max": 789,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_75f75acaddf54d298bb1f5b14ff027a8",
            "value": 789
          }
        },
        "abe386171f024ec2b129e55d57a48eef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3a18dfd271442469acd256b59dc94cc",
            "placeholder": "​",
            "style": "IPY_MODEL_27ef536f5fb54899ab0a5cbd427e308a",
            "value": " 789/789 [00:00&lt;00:00, 24.6kB/s]"
          }
        },
        "3aee636048ca41dda07c9a55f39d0184": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5677475bde1e43a1936cfc18ce80e368": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "673d30aec9bb49918f2815d2fe37d1e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "208d3bc2438345f8a2cc1f7acaa760b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75f75acaddf54d298bb1f5b14ff027a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3a18dfd271442469acd256b59dc94cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27ef536f5fb54899ab0a5cbd427e308a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "baf7d925e7404aafaca7136b7be20711": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b2ca0b2971ec44e28ccb70d51833546b",
              "IPY_MODEL_6e0419b7d2fc46298fc53753560d23bd",
              "IPY_MODEL_4178e5ff6e8c4339b7f58c5d32b9a141"
            ],
            "layout": "IPY_MODEL_634dc4c474404aeb83ef9a9d97be43dd"
          }
        },
        "b2ca0b2971ec44e28ccb70d51833546b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23e960d31dce42a88217615dce93b241",
            "placeholder": "​",
            "style": "IPY_MODEL_ab57986e44eb4e2a883771bad50b703b",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "6e0419b7d2fc46298fc53753560d23bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31e4578b04bf400eadeffccaf70cc14d",
            "max": 2630528157,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52631027568c4daeb1345c58860cf9ef",
            "value": 2630528157
          }
        },
        "4178e5ff6e8c4339b7f58c5d32b9a141": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31cec14f0ee24582a97c88f189afd2a1",
            "placeholder": "​",
            "style": "IPY_MODEL_f7e5bff20b664603b5f4cb0100e38bce",
            "value": " 2.63G/2.63G [00:20&lt;00:00, 206MB/s]"
          }
        },
        "634dc4c474404aeb83ef9a9d97be43dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23e960d31dce42a88217615dce93b241": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab57986e44eb4e2a883771bad50b703b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31e4578b04bf400eadeffccaf70cc14d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52631027568c4daeb1345c58860cf9ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "31cec14f0ee24582a97c88f189afd2a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7e5bff20b664603b5f4cb0100e38bce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}