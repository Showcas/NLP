{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Showcas/NLP/blob/main/01_3_NLTK_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN-f-sa01XjE"
      },
      "source": [
        "# Natural Language Processing with Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7N9hshV1XjG",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# we use this for progress bars\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9Fq2oij1XjH",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# Use the module by importing\n",
        "import nltk\n",
        "\n",
        "# Also, we use now a different corpus\n",
        "from nltk.corpus import brown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_T8bBlH1kJQ"
      },
      "outputs": [],
      "source": [
        "# downloading all resources\n",
        "nltk.download('brown')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlbsKkXW1XjH",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# Which categories are there?\n",
        "print(brown.categories())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urLkQtsc1XjI"
      },
      "source": [
        "\n",
        "We will train a classifier that will be able to distinguish 3 (pseudo-randomly) chosen classes:\n",
        "\n",
        "`science_fiction`, `news`, and `religion`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E90rU0ZZ1XjI",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "LABELS = {'fiction', 'news', 'religion'}\n",
        "\n",
        "# We will create a list of tuples: (<tokens>, <label>)\n",
        "\n",
        "full_corpus = [\n",
        "    (brown.words(fileids=[fileid]), label)\n",
        "    for label in LABELS\n",
        "    for fileid in brown.fileids(categories=[label])\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAdnCQH31XjI",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# Count label distribution:\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "lbl_dist = Counter(label for _, label in full_corpus)\n",
        "\n",
        "print(lbl_dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtmYDB6s1XjJ"
      },
      "source": [
        "We see from the **unbalanced** dataset that using (normal) _accuracy_ is not possible and we could not trust the result.\n",
        "\n",
        "$ N = 44 + 29 + 17 = 90 $\n",
        "\n",
        "$ P(\\text{news}) = \\frac{44}{90} = 0.49 $\n",
        "\n",
        "$ P(\\text{fiction}) = \\frac{29}{90} = 0.32 $\n",
        "\n",
        "$ P(\\text{religion}) = \\frac{17}{90} = 0.19 $\n",
        "\n",
        "So, by always choosing `news`, the accuracy will automatically be close to 50%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbyPqsbS1XjJ"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "We need a function that does the preprocessing for us. The text is already tokenized, so we do not need to this.\n",
        "\n",
        "We will do the 2 steps:\n",
        "\n",
        "1. Lowercase all tokens\n",
        "2. Remove stopwords (and punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4utspfK1XjJ",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "STOPWORDS = STOPWORDS.union(set(punctuation))\n",
        "STOPWORDS = STOPWORDS.union(set([\"''\", \"--\", \"``\"]))\n",
        "\n",
        "\n",
        "def lowercase_and_filter(tokens):\n",
        "    return [\n",
        "        t\n",
        "        for token in tokens\n",
        "        if (t := token.lower()) not in STOPWORDS\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcFDBe6q1XjK",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# example\n",
        "print(\"Unprocessed\\n\", full_corpus[0][0][:20])\n",
        "\n",
        "print(\"Preprocessed\\n\", lowercase_and_filter(full_corpus[0][0][:20]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP5WA2qp1XjK",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# We have not many words, so to reduce the vocabulary, we **stem** the tokens additionally.\n",
        "\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "STEMMER = SnowballStemmer(language='english')\n",
        "\n",
        "# example:\n",
        "\n",
        "STEMMER.stem('investigation'), STEMMER.stem('election'), STEMMER.stem('produced')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsXpFloT1XjK",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "def stem(tokens):\n",
        "    return [STEMMER.stem(token) for token in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzzdR4VL1XjK",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# example\n",
        "print(\"Unprocessed \\n\", full_corpus[0][0][:20], end=\"\\n\\n\")\n",
        "\n",
        "print(\"Preprocessed \\n\", lowercase_and_filter(full_corpus[0][0][:20]), end=\"\\n\\n\")\n",
        "\n",
        "print(\"Stemmed \\n\", stem(lowercase_and_filter(full_corpus[0][0][:20])), end=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IyYg-Qi1XjL"
      },
      "source": [
        "Some words like `said` is not reduced, so we could also use **Lemmatization**.\n",
        "\n",
        "But we know from the lecture, that Lemmatization needs _POS-Tags_ for good results, so we need to to pos-tagging first.\n",
        "\n",
        "1. POS-Tagging\n",
        "2. Lemmatization\n",
        "3. Stopword Removal\n",
        "4. Stemming (?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA_TF_6w1XjL",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# Example of POS-Tagging:\n",
        "\n",
        "nltk.pos_tag(full_corpus[0][0][:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcdTc3rl1XjL",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "nltk.pos_tag(nltk.word_tokenize(\"Don't treat me badly\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBJSMdyf1XjL",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "from nltk import WordNetLemmatizer\n",
        "\n",
        "LEMMA = WordNetLemmatizer()\n",
        "\n",
        "# example:\n",
        "\n",
        "# Without explizit POS tag (default is NOUN)\n",
        "print('Incorrect:', LEMMA.lemmatize('said'))\n",
        "\n",
        "# With correct POS tag:\n",
        "print('Correct:', LEMMA.lemmatize('said', pos='v'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkPM-q2u1XjL"
      },
      "source": [
        "Unfortunately, the `WordNetLemmatizer` needs a specific form for the pos tag, so we have to convert the tag to a compatible format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dg21zWHGsRI"
      },
      "source": [
        "#### TASK 1.5\n",
        "Implement a function to convert the position tag.\n",
        "1. Utilize wordnet.ADJ, wordnet.VERB and wordnet.ADV\n",
        "2. Tags starting with J -> wordnet.ADJ\n",
        "3. Tags starting with V -> wordnet.VERB\n",
        "4. Tags starting with R -> wordnet.ADV\n",
        "5. Tags starting with N -> wordnet.NOUN\n",
        "6. Tags starting with S -> wordnet.ADJ_SAT\n",
        "7. All other Tags should be defaulted to wordnet.NOUN.\n",
        "8. Return the converted tag.\n",
        "\n",
        "*Hint*: You can look up all the possible Tags using: `nltk.help.upenn_tagset()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDC5ObCqGsRI"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "### IMPLEMENT YOUR SOLUTION HERE ###\n",
        "# def convert_pos_tag(tag):\n",
        "\n",
        "    # return converted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaZocg3Z1XjM",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# example:\n",
        "[\n",
        "    LEMMA.lemmatize(\n",
        "        token, pos=convert_pos_tag(tag)\n",
        "    )\n",
        "    for token, tag in nltk.pos_tag(full_corpus[0][0][:20])\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnue6_A51XjM"
      },
      "source": [
        "## Combining\n",
        "\n",
        "We need to combine now all of the methods to do the preprocessing in this order:\n",
        "\n",
        "1. POS-Tagging\n",
        "2. Lemmatization\n",
        "3. Stopword Removal\n",
        "4. Stemming (?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzxKt59_1XjM",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "def preprocess(tokens):\n",
        "\n",
        "    # 1. POS-Tagging\n",
        "    with_pos = nltk.pos_tag(tokens)\n",
        "\n",
        "    # 2.1 Conversion of pos tags for lemmatizer\n",
        "    with_converted_pos = [(token, convert_pos_tag(tag)) for token, tag in with_pos]\n",
        "\n",
        "    # 2.2 Lemmatize\n",
        "    lemmatized_tokens = [LEMMA.lemmatize(token, pos=tag) for token, tag in with_converted_pos]\n",
        "\n",
        "    # 3.1 Lowercase everything\n",
        "    lowercase_tokens = [token.lower() for token in lemmatized_tokens]\n",
        "\n",
        "    # 3.2 Remove stopwords/unwanted punctuation\n",
        "    filtered_tokens = [token for token in lowercase_tokens if token not in STOPWORDS]\n",
        "\n",
        "    # 4. Stemming\n",
        "    stemmed_tokens = [STEMMER.stem(token) for token in filtered_tokens]\n",
        "\n",
        "    # Done.\n",
        "    return stemmed_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RzfHZvE1XjM",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# Now, we do the heavy-lifting (most time will be spend in the Lemmatizer—it's slow.)\n",
        "# (We will use tqdm to see the progress)\n",
        "\n",
        "preprocessed = [\n",
        "    (preprocess(tokens), label)\n",
        "    for tokens, label in tqdm(full_corpus, total=len(full_corpus), desc='Preprocessing')\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phvflM3p1XjN",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# example\n",
        "preprocessed[0][0][:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m7SktQl1XjN"
      },
      "source": [
        "## Feature Extraction\n",
        "\n",
        "For feature extraction, we use now the **count** of each of the vocabulary word. The vocabulary will be the 100 most common tokens (=lowercased stemmed lemmata without stopwords).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gggJHpdH1XjN",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "VOCABULARY = sorted(\n",
        "    token for token, _ in Counter(token for tokens, _ in preprocessed for token in tokens).most_common(100)\n",
        ")\n",
        "\n",
        "print(len(VOCABULARY))\n",
        "\n",
        "print(VOCABULARY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZ5VreeH1XjN",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "def feature_set(tokens):\n",
        "\n",
        "    features = {}\n",
        "\n",
        "    token_count = Counter(tokens)\n",
        "\n",
        "    for vocab_token in VOCABULARY:\n",
        "        features[f\"amount({vocab_token})\"] = token_count[vocab_token]\n",
        "\n",
        "    # features['text_length'] = len(tokens)\n",
        "    # features['average_token_length'] = sum(len(token) for token in tokens) / len(tokens)\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAb9A6ld1XjN",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "training_data = [\n",
        "    (feature_set(tokens), label) for tokens, label in preprocessed\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBiH72NV1XjN"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJvtkhE11XjN",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "from nltk import NaiveBayesClassifier\n",
        "\n",
        "nb = NaiveBayesClassifier.train(training_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MHNCfEy1XjO"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Now, we want to measure how well the classifier can distinguish the classes.\n",
        "\n",
        "\n",
        "But we don't have a data set for this. We already used the full `training_data` data set for training. We can't _test_ or _evaluate_ the classifier.\n",
        "\n",
        "What me **MUST** do then, **BEFOREHAND**, is _splitting_ the dataset into **two parts**: The _training_ set and _testing_ set.\n",
        "\n",
        "The *test* set is emulated to be fully and totally **UNKNOWN** to the classifier, so we are not allowed to use the full vocabulary: Only the one from the train set.\n",
        "\n",
        "1. **SPLIT** the data set\n",
        "1. Define a preprocess function\n",
        "1. Define a feature extraction function\n",
        "1. Create Vocabulary from **TRAIN** set\n",
        "1. Apply preprocessing/feature extraction for **TRAIN** set.\n",
        "1. Train classifier\n",
        "1. Apply preprocessing/feature extraction for **TEST** set.\n",
        "1. Classify **TEST** set\n",
        "1. Evaluate results!\n",
        "\n",
        "\n",
        "(We already have the preprocess/feature extraction functions.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4ffH5IZ1XjO",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# We use random 80% of the data for training\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ce9hTzTi1XjO",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "split_index = int(len(full_corpus) * 0.8)\n",
        "\n",
        "random.seed(20)\n",
        "\n",
        "shuffled = random.sample(full_corpus, len(full_corpus))\n",
        "\n",
        "train_set = shuffled[:split_index]\n",
        "test_set = shuffled[split_index:]\n",
        "\n",
        "print(f\"The train set has {len(train_set)} items, the test set {len(test_set)}\")\n",
        "\n",
        "Counter(label for _, label in train_set), Counter(label for _, label in test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMEr32B_1XjR"
      },
      "source": [
        "By randomly selecting sets there might be a problem with the label distribution. The random selection does not check if **ALL** labels are in the train and the test set.\n",
        "\n",
        "We can ensure that by splitting 80% of each label population."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sv0P0ykk1XjS",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "def train_test_split(l, amount=0.8):\n",
        "    split_index = int(len(l) * amount)\n",
        "\n",
        "    shuffled = random.sample(l, len(l))\n",
        "\n",
        "    train_set = shuffled[:split_index]\n",
        "    test_set = shuffled[split_index:]\n",
        "\n",
        "    return train_set, test_set\n",
        "\n",
        "train_set = []\n",
        "test_set = []\n",
        "\n",
        "for label in LABELS:\n",
        "    train_ids, test_ids = train_test_split(brown.fileids(categories=[label]), amount=0.8)\n",
        "    train_set.extend([\n",
        "        (brown.words(fileids=[fileid]), label) for fileid in train_ids\n",
        "    ])\n",
        "    test_set.extend([\n",
        "        (brown.words(fileids=[fileid]), label) for fileid in test_ids\n",
        "    ])\n",
        "\n",
        "\n",
        "print(f\"The train set has {len(train_set)} items, the test set {len(test_set)}\")\n",
        "\n",
        "Counter(label for _, label in train_set), Counter(label for _, label in test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyEhGTpk1XjS",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "def preprocess(tokens):\n",
        "    # 1. POS-Tagging\n",
        "    with_pos = nltk.pos_tag(tokens)\n",
        "\n",
        "    # 2.1 Conversion of pos tags for lemmatizer\n",
        "    with_converted_pos = [(token, convert_pos_tag(tag)) for token, tag in with_pos]\n",
        "\n",
        "    # 2.2 Lemmatize\n",
        "    lemmatized_tokens = [LEMMA.lemmatize(token, pos=tag) for token, tag in with_converted_pos]\n",
        "\n",
        "    # 3.1 Lowercase everything\n",
        "    lowercase_tokens = [token.lower() for token in lemmatized_tokens]\n",
        "\n",
        "    # 3.2 Remove stopwords/unwanted punctuation\n",
        "    filtered_tokens = [token for token in lowercase_tokens if token not in STOPWORDS]\n",
        "\n",
        "    # 4. Stemming\n",
        "    stemmed_tokens = [STEMMER.stem(token) for token in filtered_tokens]\n",
        "\n",
        "    # Done.\n",
        "    return stemmed_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q62sWCOm1XjS",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# We can do the same things: preprocess, feature extraction, classification\n",
        "\n",
        "train_set_preprocessed = [\n",
        "    (preprocess(tokens), label) for tokens, label in tqdm(train_set, total=len(train_set), desc='Preprocessing')\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZIa9AEB1XjT",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "TRAIN_VOCABULARY = sorted(\n",
        "    token for token, _ in Counter(token for tokens, _ in train_set_preprocessed for token in tokens).most_common(100)\n",
        ")\n",
        "\n",
        "print(len(TRAIN_VOCABULARY))\n",
        "\n",
        "print(TRAIN_VOCABULARY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBMRsV_j1XjZ",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "def feature_extraction(tokens):\n",
        "    features = {}\n",
        "\n",
        "    token_count = Counter(tokens)\n",
        "\n",
        "    for vocab_token in VOCABULARY:\n",
        "        features[f\"amount({vocab_token})\"] = token_count[vocab_token]\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhG84t6B1XjZ",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "train_data = [\n",
        "    (feature_extraction(tokens), label) for tokens, label in train_set_preprocessed\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoFYcJhp1XjZ",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "nb = NaiveBayesClassifier.train(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAPsxjiA1XjZ",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "test_data = [\n",
        "    (\n",
        "        feature_extraction(\n",
        "            preprocess(\n",
        "                tokens\n",
        "            )\n",
        "        ),\n",
        "        label\n",
        "    ) for tokens, label in tqdm(test_set, desc='Preprocessing/Feature Extraction')\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IuCAjtJ1Xja",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# We can get the accuracy directly with NLTK:\n",
        "\n",
        "nltk.classify.accuracy(nb, test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_AN-zUw1Xja",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# BUT we know, that this is skewed, so we need Precision/Recall/F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0FooAU41Xja",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "predictions = nb.classify_many([features for features, _ in test_data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkCw0wGU1Xja",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "gold = [label for _, label in test_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-cnvLU91Xja",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# Micro Average\n",
        "\n",
        "tp, fp, fn = 0, 0, 0\n",
        "\n",
        "for predicted, correct in zip(predictions, gold):\n",
        "    for label in LABELS:\n",
        "        if correct == label:\n",
        "            if predicted == label:\n",
        "                tp += 1\n",
        "            else:\n",
        "                fn += 1\n",
        "        else:\n",
        "            if predicted == label:\n",
        "                fp += 1\n",
        "            # We don't care about TN for precision/recall\n",
        "\n",
        "micro_precision = tp / (tp + fp)\n",
        "micro_recall = tp / (tp + fn)\n",
        "micro_fscore = (2 * micro_precision * micro_recall) / (micro_precision + micro_recall)\n",
        "\n",
        "print(f\"\"\"\n",
        "Micro-Precision: {micro_precision:.4f}\n",
        "Micro-Recall   : {micro_recall:.4f}\n",
        "Micro-FScore   : {micro_fscore:.4f}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhE3LeWg1Xja",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Macro Average\n",
        "\n",
        "precisions, recalls, fscores = {}, {}, {} # as dictionary so, we store it by _label_\n",
        "\n",
        "for label in LABELS:\n",
        "    tp, fp, fn = 0, 0, 0\n",
        "    for predicted, correct in zip(predictions, gold):\n",
        "        if correct == label:\n",
        "            if predicted == label:\n",
        "                tp += 1\n",
        "            else:\n",
        "                fn += 1\n",
        "        else:\n",
        "            if predicted == label:\n",
        "                fp += 1\n",
        "    p = tp / (tp + fp)\n",
        "    r = tp / (tp + fn)\n",
        "    f = (2 * p * r) / (p + r)\n",
        "\n",
        "    precisions[label] = p\n",
        "    recalls[label] = r\n",
        "    fscores[label] = f\n",
        "\n",
        "\n",
        "print(f\"Precision per Label:\")\n",
        "print('\\n'.join(['\\t' + f'{label:<10}: {value:.2f}' for label, value in precisions.items()]))\n",
        "print()\n",
        "\n",
        "print(f\"Recall per Label:\")\n",
        "print('\\n'.join(['\\t' + f'{label:<10}: {value:.2f}' for label, value in recalls.items()]))\n",
        "print()\n",
        "\n",
        "print(f\"F-Score per Label:\")\n",
        "print('\\n'.join(['\\t' + f'{label:<10}: {value:.2f}' for label, value in fscores.items()]))\n",
        "print()\n",
        "\n",
        "macro_precision = sum(precisions.values()) / len(precisions)\n",
        "macro_recall = sum(recalls.values()) / len(recalls)\n",
        "macro_fscore = sum(fscores.values()) / len(fscores)\n",
        "\n",
        "\n",
        "print(f\"\"\"\n",
        "Macro-Precision: {macro_precision:.4f}\n",
        "Macro-Recall   : {macro_recall:.4f}\n",
        "Macro-FScore   : {macro_fscore:.4f}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdsNvkI51Xja"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "HSG_NLP_EDU",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}